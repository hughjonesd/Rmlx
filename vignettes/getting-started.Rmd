---
title: "Getting Started with Rmlx"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with Rmlx}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

Rmlx provides an R interface to Apple's MLX (Machine Learning eXchange) library,
enabling GPU-accelerated array operations on Apple Silicon Macs. MLX leverages
the Metal framework for high-performance computing on Apple's GPUs.

**Important**: This package requires:
- macOS on Apple Silicon (M1, M2, M3, or later)
- MLX library installed (via Homebrew or built from source)

## System Requirements

Before using Rmlx, ensure MLX is installed:

```bash
# Using Homebrew (if available)
brew install mlx

# Or build from source
git clone https://github.com/ml-explore/mlx.git
cd mlx && mkdir build && cd build
cmake .. && make && sudo make install
```

## Creating MLX Arrays

Convert R objects to MLX arrays using `as_mlx()`:

```{r}
library(Rmlx)

# From a vector
v <- as_mlx(1:10)
print(v)

# From a matrix
m <- matrix(1:12, nrow = 3, ncol = 4)
x <- as_mlx(m)
print(x)

# Move the array to the GPU
x_gpu <- as_mlx(m, device = "gpu")
```

> **Precision note:** Numeric inputs are stored in single precision (`float32`).
> Requesting `dtype = "float64"` will downcast the input with a warning.
> Logical inputs are stored as MLX `bool` tensors (logical `NA` values are not
> supported). Complex inputs are stored as `complex64` (single-precision real
> and imaginary parts). Use base R arrays if you need double precision arithmetic.

## Lazy Evaluation

MLX arrays use lazy evaluation - operations are recorded but not computed
until needed:

```{r}
# These operations are not computed immediately
x <- as_mlx(matrix(1:100, 10, 10))
y <- as_mlx(matrix(101:200, 10, 10))
z <- x + y * 2

# Force evaluation of a specific array
mlx_eval(z)

# Or convert to R (automatically evaluates)
as.matrix(z)

# Wait for all queued work on the GPU if needed
mlx_synchronize("gpu")
```

## Arithmetic Operations

Rmlx supports standard arithmetic operators:

```{r}
x <- as_mlx(matrix(1:12, 3, 4))
y <- as_mlx(matrix(13:24, 3, 4))

# Element-wise operations
sum_xy <- x + y
diff_xy <- x - y
prod_xy <- x * y
quot_xy <- x / y
pow_xy <- x ^ 2

# Convert back to R to see results
as.matrix(sum_xy)
```

## Matrix Operations

### Matrix Multiplication

```{r}
a <- as_mlx(matrix(1:6, 2, 3))
b <- as_mlx(matrix(1:6, 3, 2))

# Matrix multiplication
c <- a %*% b
as.matrix(c)
```

### Transpose

```{r}
x <- as_mlx(matrix(1:12, 3, 4))
x_t <- t(x)
print(x_t)
```

### Cross Products

```{r}
x <- as_mlx(matrix(rnorm(20), 5, 4))

# t(x) %*% x
xtx <- crossprod(x)

# x %*% t(x)
xxt <- tcrossprod(x)
```

### Advanced decompositions

```{r}
qr_res <- qr(a)
svd_res <- svd(a)
chol_res <- chol(as_mlx(crossprod(matrix(1:6, 3, 2))))
fft_res <- fft(a)
```

### Differentiation

```{r}
loss <- function(w, x, y) {
  preds <- x %*% w
  resids <- preds - y
  sum(resids * resids) / length(y)
}

x <- as_mlx(matrix(rnorm(20), 5, 4))
y <- as_mlx(matrix(rnorm(5), 5, 1))
w <- as_mlx(matrix(0, 4, 1))

grads <- mlx_grad(loss, w, x, y)

# A small SGD loop using the module/optimizer helpers
model <- mlx_linear(4, 1, bias = FALSE)
opt <- mlx_optimizer_sgd(mlx_parameters(model), lr = 0.1)
loss_fn <- function(mod, data_x, data_y) {
  preds <- mlx_forward(mod, data_x)
  resids <- preds - data_y
  sum(resids * resids) / length(data_y)
}
for (step in 1:50) {
  mlx_train_step(model, loss_fn, opt, x, y)
}
```

## Reductions

Compute summaries across arrays:

```{r}
x <- as_mlx(matrix(1:100, 10, 10))

# Overall reductions
sum(x)
mean(x)

# Column and row means
colMeans(x)
rowMeans(x)

# Convert to R to see values
as.matrix(colMeans(x))

# Cumulative operations flatten the array in column-major order
as.vector(cumsum(x))
```

## Indexing

Subset MLX arrays similar to R:

```{r}
x <- as_mlx(matrix(1:100, 10, 10))

# Select rows and columns
x_sub <- x[1:5, 1:5]

# Select specific row
row_1 <- x[1, ]

# Select specific column
col_1 <- x[, 1]
```

## Device Management

Control whether computations run on GPU or CPU:

```{r}
# Check default device
mlx_default_device()

# Set to CPU for debugging
mlx_default_device("cpu")

# Create array on CPU
x_cpu <- as_mlx(matrix(1:12, 3, 4), device = "cpu")

# Set back to GPU
mlx_default_device("gpu")
```

Remember that numeric computations are always performed in `float32`; the CPU
mode is useful when you need to compare against base R or debug without a GPU.

## Performance Comparison

Here's a simple timing comparison for large matrix multiplication:

```{r}
n <- 1000

# R base
m1 <- matrix(rnorm(n * n), n, n)
m2 <- matrix(rnorm(n * n), n, n)
t1 <- system.time(r_result <- m1 %*% m2)

# MLX
x1 <- as_mlx(m1)
x2 <- as_mlx(m2)
mlx_eval(x1)
mlx_eval(x2)
t2 <- system.time({
  mlx_result <- x1 %*% x2
  mlx_eval(mlx_result)
  final <- as.matrix(mlx_result)
})

cat("Base R:", t1["elapsed"], "seconds\n")
cat("MLX:", t2["elapsed"], "seconds\n")
```

Note: This is an informal comparison, not a rigorous benchmark. Performance
gains depend on array size, operation type, and hardware.

## Best Practices

1. **Keep data on GPU**: Minimize transfers between R and MLX
2. **Use lazy evaluation**: Build computation graphs before evaluating
3. **Batch operations**: Combine operations before forcing evaluation
4. **Monitor memory**: GPU memory is limited; free unused arrays
5. **Start with CPU**: Use CPU device for debugging, then switch to GPU

## Limitations

Current limitations in this initial version:

- Apple Silicon only (no Intel Mac or other platforms)
- 2D arrays (matrices) are primary focus
- Limited indexing operations
- No autodiff or gradient computation (planned for future release)

## Next Steps

- Explore more complex workflows
- Combine with other R packages
- Report issues at: https://github.com/your-repo/Rmlx
