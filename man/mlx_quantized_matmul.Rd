% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn.R
\name{mlx_quantized_matmul}
\alias{mlx_quantized_matmul}
\title{Quantized Matrix Multiplication}
\usage{
mlx_quantized_matmul(
  x,
  w,
  scales = NULL,
  biases = NULL,
  transpose = TRUE,
  group_size = 64L,
  bits = 4L,
  mode = "affine",
  device = mlx_default_device()
)
}
\arguments{
\item{x}{An mlx array.}

\item{w}{An mlx array. Either:
\itemize{
\item A quantized weight matrix (uint32) from \code{\link[=mlx_quantize]{mlx_quantize()}}, or
\item An unquantized weight matrix that will be quantized automatically
}}

\item{scales}{An optional mlx array (the quantization scales). If NULL and w is
unquantized, w will be quantized automatically. Default: NULL}

\item{biases}{An optional mlx array (biases to add). For affine quantization, this
should be the quantization biases if w is pre-quantized. Default: NULL}

\item{transpose}{Whether to transpose the weight matrix. Default: TRUE}

\item{group_size}{The group size for quantization. Default: 64}

\item{bits}{The number of bits for quantization (typically 4 or 8). Default: 4}

\item{mode}{The quantization mode, either "affine" or "mxfp4". Default: "affine"}

\item{device}{Execution target: supply \code{"gpu"}, \code{"cpu"}, or an
\code{mlx_stream} created via \code{\link[=mlx_new_stream]{mlx_new_stream()}}. Default: \code{mlx_default_device()}.}
}
\value{
An mlx array with the result of the quantized matrix multiplication
}
\description{
Performs matrix multiplication with a quantized weight matrix. This operation
is essential for efficient inference with quantized models, significantly reducing
memory usage and computation time while maintaining reasonable accuracy.
}
\details{
Quantized matrix multiplication uses low-precision representations (typically 4-bit or
8-bit integers) for weights, which reduces memory footprint by up to 8x compared to
float32. The scales parameter contains the dequantization factors needed to reconstruct
approximate float values during computation.

The group_size parameter controls the granularity of quantization - smaller groups
provide better accuracy but slightly higher memory usage.

\strong{Automatic Quantization}: If only w is provided (without scales), the function will
automatically quantize w using \code{\link[=mlx_quantize]{mlx_quantize()}} before performing the multiplication.
For repeated operations, it's more efficient to pre-quantize weights once using
\code{\link[=mlx_quantize]{mlx_quantize()}} and reuse them.
}
\examples{
\dontrun{
# Automatic quantization (convenient but slower for repeated use)
x <- mlx_random_normal(c(10, 256))
w <- mlx_random_normal(c(512, 256))
result <- mlx_quantized_matmul(x, w)

# Pre-quantized weights (faster for repeated operations)
quant <- mlx_quantize(w, group_size = 64, bits = 4)
result <- mlx_quantized_matmul(x, quant$w_q, quant$scales, quant$biases)
}

}
\seealso{
\code{\link[=mlx_quantize]{mlx_quantize()}}, \code{\link[=mlx_dequantize]{mlx_dequantize()}}, \code{\link[=mlx_gather_qmm]{mlx_gather_qmm()}}
}
