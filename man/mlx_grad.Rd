% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autograd.R
\name{mlx_grad}
\alias{mlx_grad}
\alias{mlx_value_grad}
\title{Automatic differentiation for MLX functions}
\usage{
mlx_grad(f, ..., argnums = NULL, value = FALSE)

mlx_value_grad(f, ..., argnums = NULL)
}
\arguments{
\item{f}{An R function. Its arguments should be \code{mlx} objects, and its return
value must be an \code{mlx} tensor (typically a scalar loss).}

\item{...}{Arguments to pass to \code{f}. They will be coerced to \code{mlx} if needed.}

\item{argnums}{Indices (1-based) identifying which arguments to
differentiate with respect to. Defaults to all arguments.}

\item{value}{Should the function value be returned alongside gradients?
Set to \code{TRUE} to receive a list with components \code{value} and \code{grads}.}
}
\value{
When \code{value = FALSE} (default), a list of \code{mlx} tensors containing the
gradients in the same order as \code{argnums}. When \code{value = TRUE}, a list with
elements \code{value} (the function output as \code{mlx}) and \code{grads}.
}
\description{
\code{mlx_grad()} computes gradients of an R function that operates on \code{mlx}
tensors. The function must keep all differentiable computations in MLX
(e.g., via \code{as_mlx()} and MLX operators) and return an \code{mlx} object.
}
\details{
Keep the differentiated closure inside MLX operations. Coercing tensors back
to base R objects (such as \code{as.matrix()}, \code{as.numeric()}, or \code{[[} extraction)
breaks the gradient tape and results in an error.
}
\examples{
\dontrun{
loss <- function(w, x, y) {
  preds <- x \%*\% w
  resids <- preds - y
  sum(resids * resids) / length(y)
}
x <- as_mlx(matrix(rnorm(20), 5, 4))
y <- as_mlx(matrix(rnorm(5), 5, 1))
w <- as_mlx(matrix(0, 4, 1))
grad_w <- mlx_grad(loss, w, x, y)[[1]]
}
}
