% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autograd.R
\name{mlx_grad}
\alias{mlx_grad}
\alias{mlx_value_grad}
\title{Automatic differentiation for MLX functions}
\usage{
mlx_grad(f, ..., argnums = NULL, value = FALSE)

mlx_value_grad(f, ..., argnums = NULL)
}
\arguments{
\item{f}{An R function. Its arguments should be mlx objects, and its return
value must be an mlx array (typically a scalar loss).}

\item{...}{Arguments to pass to \code{f}. They will be coerced to mlx if needed.}

\item{argnums}{Indices (1-based) identifying which arguments to
differentiate with respect to. Defaults to all arguments.}

\item{value}{Should the function value be returned alongside gradients?
Set to \code{TRUE} to receive a list with components \code{value} and \code{grads}.}
}
\value{
When \code{value = FALSE} (default), a list of mlx arrays containing the
gradients in the same order as \code{argnums}. When \code{value = TRUE}, a list with
elements \code{value} (the function output as mlx) and \code{grads}.
}
\description{
\code{mlx_grad()} computes gradients of an R function that operates on mlx
arrays. The function must keep all differentiable computations in MLX
(e.g., via \code{as_mlx()} and MLX operators) and return an mlx object.
}
\details{
Keep the differentiated closure inside MLX operations. Coercing arrays back
to base R objects (such as \code{as.matrix()}, \code{as.numeric()}, or \code{[[} extraction)
breaks the gradient tape and results in an error.
}
\examples{
loss <- function(w, x, y) {
  preds <- x \%*\% w
  resids <- preds - y
  sum(resids * resids) / length(y)
}
x <- as_mlx(matrix(1:8, 4, 2))
y <- as_mlx(matrix(c(1, 3, 2, 4), 4, 1))
w <- as_mlx(matrix(0, 2, 1))
mlx_grad(loss, w, x, y)[[1]]
loss <- function(w, x) sum((x \%*\% w) * (x \%*\% w))
x <- as_mlx(matrix(1:4, 2, 2))
w <- as_mlx(matrix(c(1, -1), 2, 1))
mlx_value_grad(loss, w, x)
}
\seealso{
\url{https://ml-explore.github.io/mlx/build/html/python/transforms.html#mlx.core.grad},
\url{https://ml-explore.github.io/mlx/build/html/python/transforms.html#mlx.core.value_and_grad}
}
