% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn.R
\name{mlx_dequantize}
\alias{mlx_dequantize}
\title{Dequantize a Matrix}
\usage{
mlx_dequantize(
  w,
  scales,
  biases = NULL,
  group_size = 64L,
  bits = 4L,
  mode = "affine",
  device = mlx_default_device()
)
}
\arguments{
\item{w}{An mlx array (the quantized weight matrix)}

\item{scales}{An mlx array (the quantization scales)}

\item{biases}{An optional mlx array (the quantization biases for affine mode). Default: NULL}

\item{group_size}{The group size used during quantization. Default: 64}

\item{bits}{The number of bits used during quantization. Default: 4}

\item{mode}{The quantization mode used: "affine" or "mxfp4". Default: "affine"}

\item{device}{Execution target: supply \code{"gpu"}, \code{"cpu"}, or an
\code{mlx_stream} created via \code{\link[=mlx_new_stream]{mlx_new_stream()}}. Default: \code{mlx_default_device()}.}
}
\value{
An mlx array with the dequantized (approximate) floating-point weights
}
\description{
Reconstructs an approximate floating-point matrix from a quantized representation
produced by \code{\link[=mlx_quantize]{mlx_quantize()}}.
}
\details{
Dequantization unpacks the low-precision quantized weights and applies the scales
(and biases) to reconstruct approximate floating-point values. Note that some
precision is lost during quantization and cannot be recovered.
}
\examples{
\dontrun{
w <- mlx_random_normal(c(512, 256))
quant <- mlx_quantize(w)
w_reconstructed <- mlx_dequantize(quant$w_q, quant$scales, quant$biases)
}

}
\seealso{
\code{\link[=mlx_quantize]{mlx_quantize()}}, \code{\link[=mlx_quantized_matmul]{mlx_quantized_matmul()}}
}
