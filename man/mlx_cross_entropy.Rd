% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/losses.R
\name{mlx_cross_entropy}
\alias{mlx_cross_entropy}
\title{Cross-entropy loss}
\usage{
mlx_cross_entropy(logits, targets, reduction = c("mean", "sum", "none"))
}
\arguments{
\item{logits}{Unnormalized predictions (logits) as an mlx array.}

\item{targets}{Target class indices as an mlx array or integer vector.}

\item{reduction}{Type of reduction: "mean" (default), "sum", or "none".}
}
\value{
An mlx array containing the loss.
}
\description{
Computes cross-entropy loss for multi-class classification.
}
\examples{
# Logits for 3 samples, 4 classes
logits <- as_mlx(matrix(rnorm(12), 3, 4))
targets <- as_mlx(c(1, 3, 2))  # 0-indexed class labels
mlx_cross_entropy(logits, targets)
}
\seealso{
\href{https://ml-explore.github.io/mlx/build/html/python/nn.html#mlx.nn.losses.cross_entropy}{mlx.nn.losses.cross_entropy}
}
