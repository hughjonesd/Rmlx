% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn.R
\name{mlx_quantize}
\alias{mlx_quantize}
\title{Quantize a Matrix}
\usage{
mlx_quantize(
  w,
  group_size = 64L,
  bits = 4L,
  mode = "affine",
  device = mlx_default_device()
)
}
\arguments{
\item{w}{An mlx array (the weight matrix to quantize)}

\item{group_size}{The group size for quantization. Smaller groups provide better
accuracy but slightly higher memory. Default: 64}

\item{bits}{The number of bits for quantization (typically 4 or 8). Default: 4}

\item{mode}{The quantization mode: "affine" (with scales and biases) or "mxfp4"
(4-bit floating point with group_size=32). Default: "affine"}

\item{device}{Execution target: supply \code{"gpu"}, \code{"cpu"}, or an
\code{mlx_stream} created via \code{\link[=mlx_new_stream]{mlx_new_stream()}}. Default: \code{mlx_default_device()}.}
}
\value{
A list containing:
\item{w_q}{The quantized weight matrix (packed as uint32)}
\item{scales}{The quantization scales for dequantization}
\item{biases}{The quantization biases (NULL for symmetric mode)}
}
\description{
Quantizes a weight matrix to low-precision representation (typically 4-bit or 8-bit).
This reduces memory usage and enables faster computation during inference.
}
\details{
Quantization converts floating-point weights to low-precision integers, reducing
memory by up to 8x for 4-bit quantization. The scales (and optionally biases) are
stored to enable approximate reconstruction of the original values.
}
\examples{
\dontrun{
w <- mlx_random_normal(c(512, 256))
quant <- mlx_quantize(w, group_size = 64, bits = 4)
# Use quant$w_q, quant$scales, quant$biases with mlx_quantized_matmul()
}

}
\seealso{
\code{\link[=mlx_dequantize]{mlx_dequantize()}}, \code{\link[=mlx_quantized_matmul]{mlx_quantized_matmul()}}
}
