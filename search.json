[{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/AGENTS.html","id":"project-structure--module-organization","dir":"","previous_headings":"","what":"Project Structure & Module Organization","title":"Repository Guidelines","text":"R/ holds exported R wrappers, S3 methods, roxygen docs; mirror existing files like ops.R adding API surface. src/ contains Rcpp glue MLX (mlx_bindings.cpp, mlx_ops.cpp); keep headers sync RcppExports.R. tests/testthat/ groups unit specs domain (test-math.R, test-matmul.R); add new files test-feature.R. vignettes/getting-started.Rmd introduces workflows; update adding user-facing features. configure, DESCRIPTION, NAMESPACE manage build-time detection package metadata; configure step runs automatically install.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/AGENTS.html","id":"build-test-and-development-commands","dir":"","previous_headings":"","what":"Build, Test, and Development Commands","title":"Repository Guidelines","text":"R -q -e 'Rcpp::compileAttributes()' regenerates RcppExports touching headers .cpp. R -q -e 'devtools::document()' rebuilds NAMESPACE Rd files roxygen comments. R -q -e 'devtools::build()' creates source tarball; R -q -e 'devtools::check()' runs formal package checks. R -q -e 'devtools::test()' runs testthat suite; use R -q -e 'devtools::load_all()' rapid iteration.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/AGENTS.html","id":"coding-style--naming-conventions","dir":"","previous_headings":"","what":"Coding Style & Naming Conventions","title":"Repository Guidelines","text":"Use two-space indents R C++; keep lines 100 characters match current style. Prefer snake_case R helpers (as_mlx), S3 methods Generic.class (Math.mlx). C++ helpers follow descriptive snake_case RAII patterns; include <mlx/mlx.h> via mlx_bindings.hpp. Document R functions roxygen #' blocks; let @export drive NAMESPACE entries.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/AGENTS.html","id":"testing-guidelines","dir":"","previous_headings":"","what":"Testing Guidelines","title":"Repository Guidelines","text":"Write tests testthat tests/testthat; mirror existing structure keep scenario-focused blocks within test_that. Use CPU-friendly fixtures (small matrices) GPU CPU paths run quickly. Run R -q -e 'devtools::test()' locally; conditional skips—tests allowed fail MLX absent.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/AGENTS.html","id":"commit--pull-request-guidelines","dir":"","previous_headings":"","what":"Commit & Pull Request Guidelines","title":"Repository Guidelines","text":"Follow repository’s imperative, capitalized commit style (e.g., Add rowSums helper); keep subject lines near 70 characters. PR link issues relevant, summarize API changes, note Metal/CPU devices covered. opening PR, run R -q -e 'devtools::document()', R -q -e 'devtools::test()', R -q -e 'devtools::check()'; include notable outputs screenshots performance-sensitive work.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/AGENTS.html","id":"current-agent-notes-2025-10-22","dir":"","previous_headings":"","what":"Current Agent Notes (2025-10-22)","title":"Repository Guidelines","text":"MLX tensor creation Metal-backed work (e.g., as_mlx(), GPU tests) succeed session runs danger-full-access; restricted sandboxes block Metal device initialisation processx/callr’s kqueue() calls. restricted modes can still build via R CMD build/INSTALL roxygen2::roxygenise(load_code = roxygen2::load_source), expect MLX runtime calls bail c++ exception (unknown reason). danger-full-access, full devtools workflow (document(), test(), check()) works end--end. devtools::check() currently reports single NOTE bashism configure line 33; everything else clean. Keep workspace tidy checks: remove Rmlx_0.0.0.9000.tar.gz, temporary Rmlx.Rcheck/, local library/ installs create . Tests now pass, rely MLX availability. Avoid adding conditional skips—failures acceptable MLX absent.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"package-overview","dir":"","previous_headings":"","what":"Package Overview","title":"CLAUDE.md","text":"Rmlx R interface Apple’s MLX (Machine Learning eXchange) library GPU-accelerated array operations Apple Silicon. package provides lazy evaluation, S3 operator overloading, familiar R syntax GPU computing. Requirements: - macOS Apple Silicon (M1/M2/M3+) - MLX C/C++ library installed - Rcpp R/C++ integration","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"building-and-testing","dir":"","previous_headings":"Development Commands","what":"Building and Testing","title":"CLAUDE.md","text":"R -q -e 'Rcpp::compileAttributes()' - Generate Rcpp exports modifying C++ code (ALWAYS run first) R -q -e 'devtools::document()' - Generate documentation roxygen comments R -q -e 'devtools::load_all()' - Load package interactive development R -q -e 'devtools::build()' - Build package R -q -e 'devtools::check()' - Run R CMD check R -q -e 'devtools::test()' - Run tests","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"installing","dir":"","previous_headings":"Development Commands","what":"Installing","title":"CLAUDE.md","text":"R -q -e 'devtools::install()' - Install package locally (requires MLX)","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"mlx-installation","dir":"","previous_headings":"System Requirements","what":"MLX Installation","title":"CLAUDE.md","text":"package requires MLX headers library. Configure script searches: - /opt/homebrew/include/mlx/c/mlx.h (headers) - /opt/homebrew/lib/libmlx.dylib (library) - /usr/local/include, /usr/local/lib (alternatives) Override environment variables:","code":"export MLX_INCLUDE=/path/to/mlx/include export MLX_LIB_DIR=/path/to/mlx/lib export MLX_LIBS=\"-lmlx\""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"build-process","dir":"","previous_headings":"System Requirements","what":"Build Process","title":"CLAUDE.md","text":"configure script detects MLX writes src/Makevars cleanup script removes generated src/Makevars Build fails gracefully helpful error MLX found","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"c-core-src","dir":"","previous_headings":"Architecture","what":"C++ Core (src/)","title":"CLAUDE.md","text":"mlx_bindings.hpp/cpp - RAII wrapper, array creation, conversion, evaluation mlx_ops.cpp - Unary ops, binary ops, reductions, matrix ops, slicing init.cpp - R package registration RcppExports.cpp - Auto-generated Rcpp::compileAttributes() Key design: - MlxArray class wraps mlx_array* RAII semantics - External pointers R finalizers memory management - C++ functions exported via [[Rcpp::export]]","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"r-code-r","dir":"","previous_headings":"Architecture","what":"R Code (R/)","title":"CLAUDE.md","text":"class.R - S3 class, constructors, converters (as_mlx, .matrix.mlx, mlx_eval) ops.R - Operator overloading (Ops.mlx, %*%.mlx) stats.R - Reductions matrix helpers (sum, mean, colMeans, t, crossprod) utils.R - Print, indexing [.mlx, accessors (dim, length) device.R - Device management (mlx_default_device, mlx_synchronize) Rmlx-package.R - Package documentation RcppExports.R - Auto-generated Rcpp::compileAttributes()","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"key-features","dir":"","previous_headings":"Architecture","what":"Key Features","title":"CLAUDE.md","text":"Lazy evaluation - Operations build computation graph; evaluate mlx_eval() .matrix() S3 dispatch - Standard R syntax: +, -, *, /, ^, %*%, t(), etc. Broadcasting - NumPy-style broadcasting binary ops GPU/CPU - Default GPU; switch mlx_default_device()","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"mlx-c-api-assumptions","dir":"","previous_headings":"Architecture","what":"MLX C API Assumptions","title":"CLAUDE.md","text":"C++ code assumes MLX C API functions like: - mlx_array_from_data(), mlx_array_free(), mlx_array_eval() - mlx_array_add(), mlx_array_matmul(), mlx_array_transpose() - mlx_array_sum(), mlx_array_mean(), etc. IMPORTANT: actual MLX C API function names may differ. compilation errors occur, check MLX documentation update function names src/mlx_bindings.cpp src/mlx_ops.cpp.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"mlx-headers-not-found","dir":"","previous_headings":"Common Issues","what":"MLX headers not found","title":"CLAUDE.md","text":"Install MLX: brew install mlx (available) build source Set MLX_INCLUDE environment variable Check header path /path//include/mlx/c/mlx.h","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"wrong-mlx-function-names","dir":"","previous_headings":"Common Issues","what":"Wrong MLX function names","title":"CLAUDE.md","text":"MLX C API may different naming conventions Check #include <mlx/c/mlx.h> equivalent header Update function calls src/mlx_bindings.cpp src/mlx_ops.cpp","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"axis-confusion-row-major-vs-column-major","dir":"","previous_headings":"Common Issues","what":"Axis confusion (row-major vs column-major)","title":"CLAUDE.md","text":"R uses column-major order MLX uses row-major order Tests verify colMeans/rowMeans map correct axes tests fail, swap axis=0 axis=1 reductions","code":""},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"testing","dir":"","previous_headings":"","what":"Testing","title":"CLAUDE.md","text":"Tests tests/testthat/: - tests skip package can’t load (MLX available) - Use tolerance = 1e-6 floating-point comparisons - Compare base R results correctness Run specific test file:","code":"R -q -e 'devtools::test_file(\"tests/testthat/test-ops.R\")'"},{"path":"https://hughjonesd.github.io/Rmlx/CLAUDE.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"CLAUDE.md","text":"Roxygen2 comments R files Vignette vignettes/getting-started.Rmd Examples use \\dontrun{} since MLX required editing docs: possible, use usethis:: package commands things canonical way. array type doesn’t default constructor!","code":"R -q -e 'devtools::document()'"},{"path":"https://hughjonesd.github.io/Rmlx/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Rmlx authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with Rmlx","text":"Apple MLX (Machine Learning eXchange) Apple’s high-performance array machine-learning framework Apple Silicon, built top Metal GPU execution optimized CPU kernels. offers lazy evaluation, vectorized math, automatic differentiation, neural network building blocks (see official MLX documentation full details). Rmlx thin R layer MLX lets : Create MLX tensors R data (as_mlx()). Run GPU-accelerated math, linear algebra, FFTs, reductions familiar R syntax. Use automatic differentiation (mlx_grad(), mlx_value_grad()) optimization. Build simple models MLX modules update using SGD helpers. heavy computation stays MLX land; copy back base R call functions like .matrix(). ## System Requirements using Rmlx, ensure MLX installed:","code":"# Using Homebrew (if available) brew install mlx  # Or build from source git clone https://github.com/ml-explore/mlx.git cd mlx && mkdir build && cd build cmake .. && make && sudo make install"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"creating-mlx-arrays","dir":"Articles","previous_headings":"","what":"Creating MLX Arrays","title":"Getting Started with Rmlx","text":"Convert R objects MLX arrays using as_mlx(): Precision note: Numeric inputs stored single precision (float32). Requesting dtype = \"float64\" downcast input warning. Logical inputs stored MLX bool tensors (logical NA values supported). Complex inputs stored complex64 (single-precision real imaginary parts). Use base R arrays need double precision arithmetic.","code":"library(Rmlx) #>  #> Attaching package: 'Rmlx' #> The following object is masked from 'package:stats': #>  #>     fft #> The following objects are masked from 'package:base': #>  #>     colMeans, colSums, rowMeans, rowSums  # From a vector v <- as_mlx(1:10) print(v) #> mlx array [10] #>   dtype: float32 #>   device: gpu #>   values: #>  [1]  1  2  3  4  5  6  7  8  9 10  # From a matrix m <- matrix(1:12, nrow = 3, ncol = 4) x <- as_mlx(m) print(x) #> mlx array [3 x 4] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] [,4] #> [1,]    1    4    7   10 #> [2,]    2    5    8   11 #> [3,]    3    6    9   12  # Move the array to the GPU x_gpu <- as_mlx(m, device = \"gpu\")"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"lazy-evaluation","dir":"Articles","previous_headings":"","what":"Lazy Evaluation","title":"Getting Started with Rmlx","text":"MLX arrays use lazy evaluation - operations recorded computed needed:","code":"# These operations are not computed immediately x <- as_mlx(matrix(1:100, 10, 10)) y <- as_mlx(matrix(101:200, 10, 10)) z <- x + y * 2  # Force evaluation of a specific array mlx_eval(z)  # Or convert to R (automatically evaluates) as.matrix(z) #>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #>  [1,]  203  233  263  293  323  353  383  413  443   473 #>  [2,]  206  236  266  296  326  356  386  416  446   476 #>  [3,]  209  239  269  299  329  359  389  419  449   479 #>  [4,]  212  242  272  302  332  362  392  422  452   482 #>  [5,]  215  245  275  305  335  365  395  425  455   485 #>  [6,]  218  248  278  308  338  368  398  428  458   488 #>  [7,]  221  251  281  311  341  371  401  431  461   491 #>  [8,]  224  254  284  314  344  374  404  434  464   494 #>  [9,]  227  257  287  317  347  377  407  437  467   497 #> [10,]  230  260  290  320  350  380  410  440  470   500  # Wait for all queued work on the GPU if needed mlx_synchronize(\"gpu\")"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"arithmetic-operations","dir":"Articles","previous_headings":"","what":"Arithmetic Operations","title":"Getting Started with Rmlx","text":"Rmlx supports standard arithmetic operators:","code":"x <- as_mlx(matrix(1:12, 3, 4)) y <- as_mlx(matrix(13:24, 3, 4))  # Element-wise operations sum_xy <- x + y diff_xy <- x - y prod_xy <- x * y quot_xy <- x / y pow_xy <- x ^ 2  # Convert back to R to see results as.matrix(sum_xy) #>      [,1] [,2] [,3] [,4] #> [1,]   14   20   26   32 #> [2,]   16   22   28   34 #> [3,]   18   24   30   36"},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"matrix-multiplication","dir":"Articles","previous_headings":"Matrix Operations","what":"Matrix Multiplication","title":"Getting Started with Rmlx","text":"","code":"a <- as_mlx(matrix(1:6, 2, 3)) b <- as_mlx(matrix(1:6, 3, 2))  # Matrix multiplication c <- a %*% b as.matrix(c) #>      [,1] [,2] #> [1,]   22   49 #> [2,]   28   64"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"transpose","dir":"Articles","previous_headings":"Matrix Operations","what":"Transpose","title":"Getting Started with Rmlx","text":"","code":"x <- as_mlx(matrix(1:12, 3, 4)) x_t <- t(x) print(x_t) #> mlx array [4 x 3] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] #> [1,]    1    2    3 #> [2,]    4    5    6 #> [3,]    7    8    9 #> [4,]   10   11   12"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"cross-products","dir":"Articles","previous_headings":"Matrix Operations","what":"Cross Products","title":"Getting Started with Rmlx","text":"","code":"x <- as_mlx(matrix(rnorm(20), 5, 4)) true_w <- as_mlx(matrix(c(2, -1, 0.5, 0.25), 4, 1)) y <- x %*% true_w w <- as_mlx(matrix(0, 4, 1))  # Loss must stay entirely in MLX-land: no conversions back to base R loss <- function(theta, data_x, data_y) {   preds <- data_x %*% theta   resids <- preds - data_y   sum(resids * resids) / length(data_y) }  grads <- mlx_grad(loss, w, x, y)  # Wrong: converting to base R breaks the gradient bad_loss <- function(theta, data_x, data_y) {   preds <- as.matrix(data_x %*% theta)  # leaves MLX   resids <- preds - as.matrix(data_y)   sum(resids * resids) / nrow(resids) } try(mlx_grad(bad_loss, w, x, y)) #> Error in eval(expr, envir) :  #>   MLX autograd failed to differentiate the function: Gradient function must return an `mlx` object. Ensure your closure keeps computations in MLX or wraps the result with as_mlx(). #> Ensure all differentiable computations use MLX operations.  # A small SGD loop using the module/optimizer helpers model <- mlx_linear(4, 1, bias = FALSE)  # learns a single weight vector parameters <- mlx_parameters(model) opt <- mlx_optimizer_sgd(parameters, lr = 0.1) loss_fn <- function(mod, data_x, data_y) {   theta <- mlx_param_values(parameters)[[1]]   loss(theta, data_x, data_y) }  loss_history <- numeric(50) for (step in seq_along(loss_history)) {   step_res <- mlx_train_step(model, loss_fn, opt, x, y)   loss_history[step] <- as.vector(step_res$loss) }  # Check final loss and inspect learned parameters final_loss <- mlx_forward(model, x) residual_mse <- as.vector(mean((final_loss - y) * (final_loss - y))) residual_mse #> [1] 5.658819e-05 loss_history #>  [1] 8.308889e+00 4.272983e+00 2.288200e+00 1.283615e+00 7.582743e-01 #>  [6] 4.726674e-01 3.102017e-01 2.130744e-01 1.520021e-01 1.117421e-01 #> [11] 8.408680e-02 6.443550e-02 5.009037e-02 3.939489e-02 3.128565e-02 #> [16] 2.505323e-02 2.020860e-02 1.640565e-02 1.339452e-02 1.099170e-02 #> [21] 9.060724e-03 7.498934e-03 6.228307e-03 5.189063e-03 4.335000e-03 #> [26] 3.630075e-03 3.046037e-03 2.560509e-03 2.155664e-03 1.817213e-03 #> [31] 1.533631e-03 1.295554e-03 1.095328e-03 9.267041e-04 7.845016e-04 #> [36] 6.644622e-04 5.630297e-04 4.772589e-04 4.046760e-04 3.432218e-04 #> [41] 2.911653e-04 2.470508e-04 2.096552e-04 1.779418e-04 1.510463e-04 #> [46] 1.282235e-04 1.088608e-04 9.242821e-05 7.848036e-05 6.664058e-05  learned_w <- mlx_param_values(parameters)[[1]] as.matrix(learned_w) #>            [,1] #> [1,]  1.9941052 #> [2,] -1.0081218 #> [3,]  0.4934084 #> [4,]  0.2496413 as.matrix(true_w) #>       [,1] #> [1,]  2.00 #> [2,] -1.00 #> [3,]  0.50 #> [4,]  0.25"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"reductions","dir":"Articles","previous_headings":"","what":"Reductions","title":"Getting Started with Rmlx","text":"Compute summaries across arrays:","code":"x <- as_mlx(matrix(1:100, 10, 10))  # Overall reductions sum(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 5050 mean(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 50.5  # Column and row means colMeans(x) #> mlx array [10] #>   dtype: float32 #>   device: gpu #>   values: #>  [1]  5.5 15.5 25.5 35.5 45.5 55.5 65.5 75.5 85.5 95.5 rowMeans(x) #> mlx array [10] #>   dtype: float32 #>   device: gpu #>   values: #>  [1] 46 47 48 49 50 51 52 53 54 55  # Convert to R to see values as.matrix(colMeans(x)) #>  [1]  5.5 15.5 25.5 35.5 45.5 55.5 65.5 75.5 85.5 95.5  # Cumulative operations flatten the array in column-major order as.vector(cumsum(x)) #>   [1]    1    3    6   10   15   21   28   36   45   55   66   78   91  105  120 #>  [16]  136  153  171  190  210  231  253  276  300  325  351  378  406  435  465 #>  [31]  496  528  561  595  630  666  703  741  780  820  861  903  946  990 1035 #>  [46] 1081 1128 1176 1225 1275 1326 1378 1431 1485 1540 1596 1653 1711 1770 1830 #>  [61] 1891 1953 2016 2080 2145 2211 2278 2346 2415 2485 2556 2628 2701 2775 2850 #>  [76] 2926 3003 3081 3160 3240 3321 3403 3486 3570 3655 3741 3828 3916 4005 4095 #>  [91] 4186 4278 4371 4465 4560 4656 4753 4851 4950 5050"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"indexing","dir":"Articles","previous_headings":"","what":"Indexing","title":"Getting Started with Rmlx","text":"Subset MLX arrays similar R:","code":"x <- as_mlx(matrix(1:100, 10, 10))  # Select rows and columns x_sub <- x[1:5, 1:5]  # Select specific row row_1 <- x[1, ]  # Select specific column col_1 <- x[, 1]"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"device-management","dir":"Articles","previous_headings":"","what":"Device Management","title":"Getting Started with Rmlx","text":"Control whether computations run GPU CPU: Remember numeric computations always performed float32; CPU mode useful need compare base R debug without GPU.","code":"# Check default device mlx_default_device() #> [1] \"gpu\"  # Set to CPU for debugging mlx_default_device(\"cpu\") #> [1] \"cpu\"  # Create array on CPU x_cpu <- as_mlx(matrix(1:12, 3, 4), device = \"cpu\")  # Set back to GPU mlx_default_device(\"gpu\") #> [1] \"gpu\""},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"performance-comparison","dir":"Articles","previous_headings":"","what":"Performance Comparison","title":"Getting Started with Rmlx","text":"’s simple timing comparison large matrix multiplication: Note: informal comparison, rigorous benchmark. Performance gains depend array size, operation type, hardware.","code":"n <- 1000  # R base m1 <- matrix(rnorm(n * n), n, n) m2 <- matrix(rnorm(n * n), n, n) t1 <- system.time(r_result <- m1 %*% m2)  # MLX x1 <- as_mlx(m1) x2 <- as_mlx(m2) mlx_eval(x1) mlx_eval(x2) t2 <- system.time({   mlx_result <- x1 %*% x2   mlx_eval(mlx_result)   final <- as.matrix(mlx_result) })  cat(\"Base R:\", t1[\"elapsed\"], \"seconds\\n\") #> Base R: 0.403 seconds cat(\"MLX:\", t2[\"elapsed\"], \"seconds\\n\") #> MLX: 0.012 seconds"},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Getting Started with Rmlx","text":"Keep data GPU: Minimize transfers R MLX Use lazy evaluation: Build computation graphs evaluating Batch operations: Combine operations forcing evaluation Monitor memory: GPU memory limited; free unused arrays Start CPU: Use CPU device debugging, switch GPU","code":""},{"path":"https://hughjonesd.github.io/Rmlx/articles/getting-started.html","id":"limitations","dir":"Articles","previous_headings":"","what":"Limitations","title":"Getting Started with Rmlx","text":"Current limitations initial version: Apple Silicon (Intel Mac platforms) 2D arrays (matrices) primary focus Limited indexing operations autodiff gradient computation (planned future release)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Linear Regression with MLX","text":"vignette demonstrates linear regression using Rmlx, based MLX linear regression example. ’ll train linear model using automatic differentiation stochastic gradient descent (SGD) GPU-accelerated arrays.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"problem-setup","dir":"Articles","previous_headings":"","what":"Problem Setup","title":"Linear Regression with MLX","text":"’ll create synthetic data linear regression high dimensionality: generate: - random “true” weight vector w_star dimension 100 - random design matrix X 1000 examples × 100 features - Noisy labels y = X @ w_star + small_noise","code":"library(Rmlx) #>  #> Attaching package: 'Rmlx' #> The following object is masked from 'package:stats': #>  #>     fft #> The following objects are masked from 'package:base': #>  #>     colMeans, colSums, rowMeans, rowSums  # Problem metadata num_features <- 100 num_examples <- 1000 num_iters <- 10000  # iterations of SGD lr <- 0.01          # learning rate for SGD  # Set seed for reproducibility set.seed(42)  # True parameters (what we're trying to learn) w_star <- mlx_rand_normal(c(num_features, 1))  # Input examples (design matrix) X <- mlx_rand_normal(c(num_examples, num_features))  # Noisy labels eps <- 1e-2 * mlx_rand_normal(c(num_examples, 1)) y <- X %*% w_star + eps"},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"define-the-loss-function","dir":"Articles","previous_headings":"","what":"Define the Loss Function","title":"Linear Regression with MLX","text":"mean squared error loss standard choice regression: loss measures well parameters w predict labels. Lower loss means better predictions.","code":"# Define loss function loss_fn <- function(w) {   preds <- X %*% w   residuals <- preds - y   0.5 * mean(residuals * residuals) }"},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"automatic-differentiation","dir":"Articles","previous_headings":"","what":"Automatic Differentiation","title":"Linear Regression with MLX","text":"Rmlx provides mlx_grad() compute gradients via automatic differentiation. computes gradient loss respect parameters:","code":"# Get the gradient function grad_fn <- function(w) {   mlx_grad(loss_fn, w)[[1]] }"},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"training-loop-with-sgd","dir":"Articles","previous_headings":"","what":"Training Loop with SGD","title":"Linear Regression with MLX","text":"train repeatedly computing gradients updating parameters: iteration: 1. Compute gradient loss respect w 2. Update parameters using gradient step 3. Force evaluation prevent computation graph growing unbounded 4. Monitor progress printing loss every 1000 iterations","code":"# Initialize parameters randomly w <- 1e-2 * mlx_rand_normal(c(num_features, 1))  # Training loop for (i in seq_len(num_iters)) {   # Compute gradient   grad <- grad_fn(w)    # Update parameters: w = w - lr * grad   w <- w - lr * grad    # Evaluate to avoid building up computation graph   mlx_eval(w)    # Print progress   if (i %% 1000 == 0) {     loss <- loss_fn(w)     cat(\"Iteration\", i, \"- Loss:\", as.vector(loss), \"\\n\")   } } #> Iteration 1000 - Loss: 8.351099e-05  #> Iteration 2000 - Loss: 4.679277e-05  #> Iteration 3000 - Loss: 4.679227e-05  #> Iteration 4000 - Loss: 4.679227e-05  #> Iteration 5000 - Loss: 4.679227e-05  #> Iteration 6000 - Loss: 4.679227e-05  #> Iteration 7000 - Loss: 4.679227e-05  #> Iteration 8000 - Loss: 4.679227e-05  #> Iteration 9000 - Loss: 4.679227e-05  #> Iteration 10000 - Loss: 4.679227e-05"},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"evaluation","dir":"Articles","previous_headings":"","what":"Evaluation","title":"Linear Regression with MLX","text":"training, evaluate well learned parameters match true parameters: measure two things: - Final loss: well model fits data - Parameter error: close learned weights true weights","code":"# Compute final loss final_loss <- loss_fn(w) cat(\"Final loss:\", as.vector(final_loss), \"\\n\") #> Final loss: 4.679227e-05  # Compute error between learned and true parameters error <- w - w_star error_norm <- sqrt(sum(error * error)) cat(\"||w - w*|| =\", as.vector(error_norm), \"\\n\") #> ||w - w*|| = 0.00322592  # Convert to R for detailed inspection w_learned <- as.matrix(w) w_true <- as.matrix(w_star)  # Compare first few parameters print(cbind(learned = w_learned[1:10, 1], true = w_true[1:10, 1])) #>            learned         true #>  [1,]  1.536847711  1.537007570 #>  [2,] -1.927076697 -1.927428961 #>  [3,] -0.765715957 -0.765168250 #>  [4,] -1.448432326 -1.448189855 #>  [5,]  1.477905750  1.477726102 #>  [6,]  0.242855221  0.242581472 #>  [7,] -0.117670946 -0.117731288 #>  [8,]  0.007820362  0.008590168 #>  [9,] -0.157147139 -0.157259166 #> [10,]  0.370247990  0.369596273"},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"device-selection","dir":"Articles","previous_headings":"","what":"Device Selection","title":"Linear Regression with MLX","text":"default, computations run GPU speed. Switch CPU needed:","code":"# Use CPU (useful for debugging) mlx_default_device(\"cpu\") #> [1] \"cpu\"  # Or back to GPU mlx_default_device(\"gpu\") #> [1] \"gpu\""},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"related-examples","dir":"Articles","previous_headings":"","what":"Related Examples","title":"Linear Regression with MLX","text":"advanced topics, see: - getting started vignette basic Rmlx usage - performance benchmarks vignette timing comparisons - original MLX linear regression example Python","code":""},{"path":"https://hughjonesd.github.io/Rmlx/articles/linear-regression.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"Linear Regression with MLX","text":"computations use single-precision floating point (float32) gradient computation entirely automatic—hand-coded derivatives Large computation graphs can accumulate; use mlx_eval() force evaluation complex models, consider using MLX modules (see getting started vignette)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/articles/performance.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Performance Benchmarks","text":"vignette compares runtime core operations base R Rmlx.","code":"library(Rmlx) #>  #> Attaching package: 'Rmlx' #> The following object is masked from 'package:stats': #>  #>     fft #> The following objects are masked from 'package:base': #>  #>     colMeans, colSums, rowMeans, rowSums library(bench) library(dplyr) #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union library(ggplot2)"},{"path":"https://hughjonesd.github.io/Rmlx/articles/performance.html","id":"matrix-addition","dir":"Articles","previous_headings":"","what":"Matrix addition","title":"Performance Benchmarks","text":"","code":"mat_sizes <- c(256, 512, 1024) add_results <- bind_rows(lapply(mat_sizes, function(n) {   A_r <- matrix(runif(n * n), n, n)   B_r <- matrix(runif(n * n), n, n)   A_mlx <- as_mlx(A_r)   B_mlx <- as_mlx(B_r)    mb <- bench::mark(     base = { A_r + B_r },     rmlx = { as.matrix(A_mlx + B_mlx) },     iterations = 5,     check = FALSE   )   mb$size <- n   mb }))  ggplot(add_results, aes(x = factor(size), y = as.numeric(median),                          fill = as.character(expression))) +   geom_col(position = \"dodge\") +   labs(     title = \"Matrix addition timing (median)\",     x = \"Matrix dimension\",     y = \"Median time\",     fill = \"Implementation\"   )"},{"path":"https://hughjonesd.github.io/Rmlx/articles/performance.html","id":"matrix-multiplication","dir":"Articles","previous_headings":"","what":"Matrix multiplication","title":"Performance Benchmarks","text":"","code":"matmul_results <- bind_rows(lapply(mat_sizes, function(n) {   A_r <- matrix(runif(n * n), n, n)   B_r <- matrix(runif(n * n), n, n)   A_mlx <- as_mlx(A_r)   B_mlx <- as_mlx(B_r)    mb <- bench::mark(     base = { A_r %*% B_r },     rmlx = { as.matrix(A_mlx %*% B_mlx) },     iterations = 3,     check = FALSE   )   mb$size <- n   mb }))  ggplot(matmul_results, aes(x = factor(size), y = as.numeric(median),                          fill = as.character(expression))) +   geom_col(position = \"dodge\") +   labs(     title = \"Matrix multiplication timing (median)\",     x = \"Matrix dimension\",     y = \"Median time\",     fill = \"Implementation\"   )"},{"path":"https://hughjonesd.github.io/Rmlx/articles/performance.html","id":"linear-system-solves","dir":"Articles","previous_headings":"","what":"Linear system solves","title":"Performance Benchmarks","text":"","code":"solve_results <- bind_rows(lapply(mat_sizes, function(n) {   A_r <- matrix(rnorm(n * n), n, n)   A_r <- crossprod(A_r) + diag(n) * 1e-3   b_r <- matrix(rnorm(n), n, 1)   A_mlx <- as_mlx(A_r)   b_mlx <- as_mlx(b_r)    mb <- bench::mark(     base = { solve(A_r, b_r) },     rmlx = { as.matrix(solve(A_mlx, b_mlx)) },     iterations = 3,     check = FALSE   )   mb$size <- n   mb }))  ggplot(solve_results, aes(x = factor(size), y = as.numeric(median),                          fill = as.character(expression))) +   geom_col(position = \"dodge\") +   labs(     title = \"Linear solve timing (median)\",     x = \"Matrix dimension\",     y = \"Median time\",     fill = \"Implementation\"   )"},{"path":"https://hughjonesd.github.io/Rmlx/articles/performance.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"Performance Benchmarks","text":"Numeric data resides single-precision (float32) stored mlx arrays; logical data uses MLX bool, complex data uses complex64. comparing timings, make sure MLX configured GPU execution consider calling mlx_synchronize(\"gpu\") reading results avoid asynchronous delays. exact speedups vary depending problem size, GPU available, whether GPU already warm.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Hugh-Jones. Author, maintainer.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hugh-Jones D (2025). Rmlx: R Interface Apple's MLX Arrays (GPU-Accelerated Apple Silicon). R package version 0.0.0.9000, https://hughjonesd.github.io/Rmlx/.","code":"@Manual{,   title = {Rmlx: R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)},   author = {David Hugh-Jones},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://hughjonesd.github.io/Rmlx/}, }"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"rmlx","dir":"","previous_headings":"","what":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"R interface Apple’s MLX (Machine Learning eXchange) library GPU-accelerated array operations Apple Silicon.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Rmlx provides R interface Apple’s MLX framework, enabling high-performance GPU computing Apple Silicon (M1, M2, M3+) using Metal backend. package implements lazy evaluation familiar R syntax S3 method dispatch. Status: Phase 1 implementation complete (arrays, operations, evaluation, tests, documentation). Phase 2 (autodiff, optimizers) yet implemented.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"requirements","dir":"","previous_headings":"","what":"Requirements","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"macOS Apple Silicon (M1/M2/M3 later) MLX C/C++ library installed R >= 4.1.0 Rcpp >= 1.0.10","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"install-mlx","dir":"","previous_headings":"Installation","what":"Install MLX","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"First, install MLX library:","code":"# Option 1: Homebrew (if available) brew install mlx  # Option 2: Build from source git clone https://github.com/ml-explore/mlx.git cd mlx mkdir build && cd build cmake .. make sudo make install"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"install-rmlx","dir":"","previous_headings":"Installation","what":"Install Rmlx","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"","code":"# Install from source devtools::install()  # Or with custom MLX paths Sys.setenv(MLX_INCLUDE = \"/path/to/mlx/include\") Sys.setenv(MLX_LIB_DIR = \"/path/to/mlx/lib\") devtools::install()"},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"lazy-evaluation","dir":"","previous_headings":"Features","what":"Lazy Evaluation","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Operations recorded executed explicitly evaluated:","code":"library(Rmlx) #>  #> Attaching package: 'Rmlx' #> The following object is masked from 'package:stats': #>  #>     fft #> The following objects are masked from 'package:base': #>  #>     colMeans, colSums, rowMeans, rowSums  x <- as_mlx(matrix(1:100, 10, 10)) y <- as_mlx(matrix(101:200, 10, 10))  # Lazy - not computed yet z <- x + y * 2  # Force evaluation mlx_eval(z)  # Or convert to R (automatically evaluates) result <- as.matrix(z)  # Wait for queued GPU work (useful when timing) mlx_synchronize(\"gpu\")  # Simple aggregate checks sum(z) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 35150 mean(z) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 351.5"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"arithmetic-operations","dir":"","previous_headings":"Features","what":"Arithmetic Operations","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Standard R operators work seamlessly:","code":"x <- as_mlx(matrix(1:12, 3, 4)) y <- as_mlx(matrix(13:24, 3, 4))  # Element-wise operations sum_xy <- x + y diff_xy <- x - y prod_xy <- x * y quot_xy <- x / y pow_xy <- x ^ 2  # Comparisons lt <- x < y eq <- x == y  # Bring results back to R as.matrix(sum_xy) #>      [,1] [,2] [,3] [,4] #> [1,]   14   20   26   32 #> [2,]   16   22   28   34 #> [3,]   18   24   30   36 as.matrix(lt) #>      [,1] [,2] [,3] [,4] #> [1,] TRUE TRUE TRUE TRUE #> [2,] TRUE TRUE TRUE TRUE #> [3,] TRUE TRUE TRUE TRUE"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"matrix-operations","dir":"","previous_headings":"Features","what":"Matrix Operations","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Advanced decompositions mirror base R:","code":"a <- as_mlx(matrix(1:6, 2, 3)) b <- as_mlx(matrix(1:6, 3, 2))  # Matrix multiplication c <- a %*% b as.matrix(c) #>      [,1] [,2] #> [1,]   22   49 #> [2,]   28   64"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"random-sampling","dir":"","previous_headings":"Features","what":"Random Sampling","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"","code":"random_tensor <- mlx_rand_uniform(c(512, 512), min = -1, max = 1) random_tensor #> mlx array [512 x 512] #>   dtype: float32 #>   device: gpu #>   (262144 elements, not shown)"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"data-transformations","dir":"","previous_headings":"Features","what":"Data Transformations","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Common ranking helpers available mlx_* prefix; note MLX indices zero-based.","code":"scores <- as_mlx(c(0.1, 0.7, 0.4, 0.9)) as.matrix(mlx_sort(scores)) #> [1] 0.1 0.4 0.7 0.9 as.matrix(mlx_topk(scores, 2)) #> [1] 0.7 0.9 as.matrix(mlx_argmax(scores)) #> [1] 3 qr_res <- qr(a) svd_res <- svd(a) chol_res <- chol(as_mlx(crossprod(matrix(1:6, 3, 2)))) fft_res <- fft(a)  # Inspect outputs qr_res$Q #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>            [,1]       [,2] #> [1,] -0.4472135 -0.8944272 #> [2,] -0.8944272  0.4472136 svd_res$d #> [1] 9.5255181 0.5143006 as.matrix(chol_res) #>          [,1]     [,2] #> [1,] 3.741657 8.552360 #> [2,] 0.000000 1.963962"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"differentiation","dir":"","previous_headings":"Features","what":"Differentiation","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"","code":"loss <- function(w, x, y) {   preds <- x %*% w   resids <- preds - y   sum(resids * resids) / length(y) }  x <- as_mlx(matrix(rnorm(20), 5, 4)) y <- as_mlx(matrix(rnorm(5), 5, 1)) w <- as_mlx(matrix(0, 4, 1))  grads <- mlx_grad(loss, w, x, y)  # Inspect gradient as.matrix(grads[[1]]) #>             [,1] #> [1,] 0.014563746 #> [2,] 1.036490083 #> [3,] 1.091995597 #> [4,] 0.007043276  # Simple SGD loop model <- mlx_linear(4, 1, bias = FALSE) opt <- mlx_optimizer_sgd(mlx_parameters(model), lr = 0.1) loss_fn <- function(mod, data_x, data_y) {   preds <- mlx_forward(mod, data_x)   resids <- preds - data_y   sum(resids * resids) / length(data_y) } for (step in 1:50) {   mlx_train_step(model, loss_fn, opt, x, y) }  # Check final loss final_loss <- mlx_forward(model, x) mean((final_loss - y) * (final_loss - y)) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 0.3989661"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"reductions","dir":"","previous_headings":"Features","what":"Reductions","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"","code":"x <- as_mlx(matrix(1:100, 10, 10))  # Overall reductions sum(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 5050 mean(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 50.5  # Column/row means colMeans(x) #> mlx array [10] #>   dtype: float32 #>   device: gpu #>   values: #>  [1]  5.5 15.5 25.5 35.5 45.5 55.5 65.5 75.5 85.5 95.5 rowMeans(x) #> mlx array [10] #>   dtype: float32 #>   device: gpu #>   values: #>  [1] 46 47 48 49 50 51 52 53 54 55  # Cumulative operations flatten column-major as.vector(cumsum(x)) #>   [1]    1    3    6   10   15   21   28   36   45   55   66   78   91  105  120 #>  [16]  136  153  171  190  210  231  253  276  300  325  351  378  406  435  465 #>  [31]  496  528  561  595  630  666  703  741  780  820  861  903  946  990 1035 #>  [46] 1081 1128 1176 1225 1275 1326 1378 1431 1485 1540 1596 1653 1711 1770 1830 #>  [61] 1891 1953 2016 2080 2145 2211 2278 2346 2415 2485 2556 2628 2701 2775 2850 #>  [76] 2926 3003 3081 3160 3240 3321 3403 3486 3570 3655 3741 3828 3916 4005 4095 #>  [91] 4186 4278 4371 4465 4560 4656 4753 4851 4950 5050"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"indexing","dir":"","previous_headings":"Features","what":"Indexing","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"","code":"x <- as_mlx(matrix(1:100, 10, 10))  # Subset x[1:5, 1:5] #> mlx array [5 x 5] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1   11   21   31   41 #> [2,]    2   12   22   32   42 #> [3,]    3   13   23   33   43 #> [4,]    4   14   24   34   44 #> [5,]    5   15   25   35   45 x[1, ] #> mlx array [1 x 10] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #> [1,]    1   11   21   31   41   51   61   71   81    91 x[, 1] #> mlx array [10 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>       [,1] #>  [1,]    1 #>  [2,]    2 #>  [3,]    3 #>  [4,]    4 #>  [5,]    5 #>  [6,]    6 #>  [7,]    7 #>  [8,]    8 #>  [9,]    9 #> [10,]   10"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"device-management","dir":"","previous_headings":"Features","what":"Device Management","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Precision note: Numeric inputs stored float32. Requests dtype = \"float64\" downcast warning. Logical inputs stored MLX bool tensors (logical NA values supported). Complex inputs stored complex64 (single-precision real/imaginary parts). Use base R arrays require double precision arithmetic.","code":"# Check/set default device mlx_default_device()           # \"gpu\" #> [1] \"gpu\" mlx_default_device(\"cpu\")      # Switch to CPU #> [1] \"cpu\" mlx_default_device(\"gpu\")      # Back to GPU #> [1] \"gpu\"  # Create on specific device x_gpu <- as_mlx(matrix(1:12, 3, 4), device = \"gpu\") x_cpu <- as_mlx(matrix(1:12, 3, 4), device = \"cpu\")"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"data-types","dir":"","previous_headings":"","what":"Data Types","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Supported dtype: float32 numeric data (default) bool logical data","code":"x_f32 <- as_mlx(matrix(1:12, 3, 4), dtype = \"float32\") logical_mat <- as_mlx(matrix(c(TRUE, FALSE, TRUE, TRUE), 2, 2))"},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Package documentation: ?Rmlx Getting started vignette: vignette(\"getting-started\", package = \"Rmlx\") Function help: ?as_mlx, ?mlx_eval, etc.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/index.html","id":"testing","dir":"","previous_headings":"","what":"Testing","title":"R Interface to Apple's MLX Arrays (GPU-Accelerated on Apple Silicon)","text":"Tests use testthat compare base R results: Tests skip gracefully MLX available package fails load.","code":"# Run all tests devtools::test()  # Run specific test file devtools::test_file(\"tests/testthat/test-ops.R\")"},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"high-level-scope-for-the-agent","dir":"","previous_headings":"","what":"High-level scope (for the agent)","title":"Project: Rmlx","text":"Language: R (+ C++ via Rcpp). OS: macOS Apple Silicon (Metal backend). Cross-platform explicitly scope now. Dependency: Apple MLX C/C++ library (present system). ’ll vendor MLX; ’ll detect link . Core object: S3 class mlx wrapping external pointer MLX array. Semantics: lazy default. .matrix.mlx() (conversions) force evaluation; otherwise, users call mlx_eval(x). Operator overloading: arithmetic, matrix algebra, comparisons; plus targeted stats helpers (colMeans.mlx, rowMeans.mlx, crossprod.mlx, tcrossprod.mlx, t.mlx, sum.mlx, mean.mlx). Phase 1: low-level arrays + ops, evaluation, conversions, basic reductions, tests, doc. (Autodiff/optimizers reserved later phase, implemented now.)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestones","dir":"","previous_headings":"","what":"Milestones","title":"Project: Rmlx","text":"Scaffold & toolchain C++ core: array handle, create/convert/eval Binary ops & reductions Matrix algebra & helpers R S3 class + operator overloading Indexing, printing, diagnostics Device/stream management Docs, examples, tests Build, local check, packaging milestone lists atomic tasks.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-1--scaffold--toolchain","dir":"","previous_headings":"","what":"Milestone 1 — Scaffold & toolchain","title":"Project: Rmlx","text":"T1. Create package skeleton Create standard R package layout: Acceptance: R CMD build produces tarball; R CMD check runs (expect skips missing MLX T3/T4 add detection). T2. DESCRIPTION Minimal fields: Package: Rmlx Type: Package Title: R Interface Apple's MLX Arrays (GPU-Accelerated Apple Silicon) Version: 0.0.0.9000 Authors@R: person(\"First\",\"Last\", role=c(\"aut\",\"cre\"), email=\"@example.com\") Description: S3 class 'mlx' backed Apple MLX arrays lazy GPU ops via Rcpp. License: MIT + file LICENSE Encoding: UTF-8 Depends: R (>= 4.1.0) Imports: Rcpp (>= 1.0.10) LinkingTo: Rcpp Suggests: testthat (>= 3.0.0), knitr, rmarkdown Config/testthat/edition: 3 SystemRequirements: MLX (Apple Machine Learning eXchange) C/C++ headers library; macOS Apple Silicon Acceptance: R CMD check reads DESCRIPTION cleanly. T3. NAMESPACE Start : ’ll also register %*% mlx via setMethod pattern R (see T17). T4. Build-time MLX detection (configure) Implement POSIX shell configure : Looks MLX headers & libs (probe typical locations: /opt/homebrew/include, /opt/homebrew/lib, /usr/local/include, /usr/local/lib, xcrun -sdk macosx --show-sdk-path). Allows env overrides: MLX_INCLUDE, MLX_LIB_DIR, MLX_LIBS (e.g., -lmlx -lc++) Writes src/Makevars PKG_CPPFLAGS including -... PKG_LIBS including -L... -lmlx. Add helpful error found: write small header check fail message telling user install MLX (Homebrew tap Apple docs). Acceptance: R CMD INSTALL . fails gracefully MLX missing; succeeds include+lib provided. Gotcha: keep Makevars minimal platform-guarded; hardcode Intel paths.","code":"Rmlx/   R/   src/   inst/   man/   tests/testthat/   vignettes/   DESCRIPTION   NAMESPACE   .Rbuildignore   .gitignore   configure     # POSIX shell   cleanup       # optional useDynLib(Rmlx, .registration = TRUE) importFrom(Rcpp, sourceCpp) S3method(print, mlx) S3method(as.matrix, mlx) S3method(colMeans, mlx) S3method(rowMeans, mlx) S3method(t, mlx) S3method(sum, mlx) S3method(mean, mlx) S3method(crossprod, mlx) S3method(tcrossprod, mlx) S3method(Ops, mlx) S3method(MatMult, mlx)       # custom for %*% (see T17)"},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-2--c-core-array-handle-createconverteval","dir":"","previous_headings":"","what":"Milestone 2 — C++ core: array handle, create/convert/eval","title":"Project: Rmlx","text":"T5. Define C++ wrapper class & finalizer File: src/mlx_bindings.cpp, plus header src/mlx_bindings.hpp. Create thin RAII wrapper around MLX array handle (consult MLX C API names; assume types like mlx_array*): Expose external pointer R finalizer deletes MlxArray. Acceptance: Creating garbage-collecting mlx object leak (use valgrind locally possible). T6. Create MLX array R data Rcpp-exposed functions (C++): SEXP cpp_mlx_from_numeric(SEXP x, SEXP dim, SEXP dtype, SEXP device); Inputs: x NumericVector (contiguous), dim IntegerVector, dtype string (“float32”/“float64”), device string (“gpu”/“cpu”). Create MLX array given shape copy host data. SEXP cpp_mlx_empty(SEXP dim, SEXP dtype, SEXP device); Acceptance: as_mlx(matrix(...)) returns mlx matching shape dtype. T7. Copy MLX array back R Function: SEXP cpp_mlx_to_numeric(SEXP x); Ensure evaluation first (see T8). copy NumericVector (column-major like R). Acceptance: .matrix.mlx(x) yields identical values input roundtrip. T8. Evaluation Implement: void cpp_mlx_eval(SEXP x); (force compute & sync). Design: store “needs_eval” flag inside wrapper, rely MLX’s state; call MLX eval array current graph root. R side: mlx_eval(x) calls cpp_mlx_eval. .matrix.mlx() must call mlx_eval() copying. Acceptance: Composed lazy ops compute evaluated converted.","code":"struct MlxArray {   mlx_array* ptr;   MlxArray();                       // null   explicit MlxArray(mlx_array* p);  // takes ownership   ~MlxArray();                      // calls mlx_array_free(ptr)   MlxArray(const MlxArray&) = delete;   MlxArray& operator=(const MlxArray&) = delete;   MlxArray(MlxArray&&) noexcept;   MlxArray& operator=(MlxArray&&) noexcept; };"},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-3--binary-ops--reductions","dir":"","previous_headings":"","what":"Milestone 3 — Binary ops & reductions","title":"Project: Rmlx","text":"T9. Elementwise unary ops C++ functions: neg, abs, sqrt, exp, log, sin, cos, etc. Signature pattern: SEXP cpp_mlx_unary(SEXP x, std::string op); // op ∈ {\"neg\",\"exp\",\"log\",...} Implementation maps MLX C API unary ops; output matches input dtype unless op requires float. T10. Elementwise binary ops Support: + - * / ^ comparisons < <= > >= == !=. Signature: SEXP cpp_mlx_binary(SEXP x, SEXP y, std::string op); Broadcasting rules: Follow MLX’s broadcasting (like NumPy). Validate shapes; throw R error incompatible shapes. scalar RHS/LHS, allow numeric logical scalars (wrap 0-d/1-d MLX arrays handle scalar op MLX supports). T11. Reductions Support: sum, mean (overall along axes), min, max. Signatures: SEXP cpp_mlx_reduce(SEXP x, std::string op); // full reduction -> scalar mlx SEXP cpp_mlx_reduce_axis(SEXP x, std::string op, int axis, bool keepdims); R wrappers provide sum.mlx, mean.mlx, helper colMeans.mlx / rowMeans.mlx built axis reductions. Acceptance: Numerically matches R within tolerance random small arrays.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-4--matrix-algebra--helpers","dir":"","previous_headings":"","what":"Milestone 4 — Matrix algebra & helpers","title":"Project: Rmlx","text":"T12. Transpose, reshape SEXP cpp_mlx_transpose(SEXP x); SEXP cpp_mlx_reshape(SEXP x, SEXP new_dim); T13. Matrix multiply SEXP cpp_mlx_matmul(SEXP , SEXP b); Accept shapes: (m×k) %*% (k×n) → (m×n); vectors meaningful (k)×(k) → scalar. Use MLX matmul op; ensure float32/float64 supported. T14. Crossprod & tcrossprod Implement top matmul + transpose, exploit MLX fused ops available later. R wrappers: crossprod.mlx(x, y = NULL) → t(x) %*% (y %||% x) tcrossprod.mlx(x, y = NULL) → x %*% t(y %||% x) T15. t.mlx, colMeans.mlx, rowMeans.mlx t.mlx → transpose colMeans.mlx → mean(x, axis=0) assuming R column-major (verify axis mapping; likely axis=0 == rows; explicit test). rowMeans.mlx → mean(x, axis=1) (ditto). Document axis semantics clearly.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-5--r-s3-class--operator-overloading","dir":"","previous_headings":"","what":"Milestone 5 — R S3 class + operator overloading","title":"Project: Rmlx","text":"T16. Class constructors & converters (R) File: R/class.R new_mlx <- function(ptr, dim, dtype, device) { structure(list(ptr=ptr, dim=dim, dtype=dtype, device=device), class=\"mlx\") } as_mlx <- function(x, dtype=c(\"float32\",\"float64\"), device=c(\"gpu\",\"cpu\")) Coerce matrix/array/numeric MLX via cpp_mlx_from_numeric. .matrix.mlx <- function(x, ...) { mlx_eval(x); cpp_mlx_to_numeric(x$ptr) |> structure(dim=x$dim) } mlx_eval <- function(x) { cpp_mlx_eval(x$ptr); invisible(x) } .mlx <- function(x) inherits(x, \"mlx\") T17. Operator overloading File: R/ops.R Define S3 method Ops.mlx dispatch elementwise + comparisons. Helper R wrappers call C++: .mlx_unary <- function(x, op) new_mlx(cpp_mlx_unary(x$ptr, op), x$dim, x$dtype, x$device) .mlx_binary <- function(x, y, op) new_mlx(cpp_mlx_binary(x$ptr, y$ptr, op), broadcast_dim(x,y), promote_dtype(x,y), common_device(x,y)) Matrix multiply %*% base R %*% primitive; define S3 generic shim: Register .onLoad: T18. Stats helpers File: R/stats.R sum.mlx, mean.mlx → reductions (full). colMeans.mlx, rowMeans.mlx → axis reductions + drop=TRUE return 1D mlx (dim length 1 removed) keepdims + reshape. t.mlx → transpose wrapper. crossprod.mlx, tcrossprod.mlx. T19. Class utilities File: R/utils.R print.mlx → show shape, dtype, device, lazy/evaluated flag; show small preview (e.g., 6×6) evaluating small slice (optionally evaluate fully size small). str.mlx → concise structure. Acceptance: Arithmetic, comparisons, %*%, col/row means reductions work mlx/regular R objects, producing mlx user calls .matrix() mlx_eval().","code":"Ops.mlx <- function(e1, e2 = NULL) {   op <- .Generic   if (is.null(e2)) {     # unary ops: \"+\" (no-op), \"-\" (neg)     if (op == \"+\") return(e1)     if (op == \"-\") return(.mlx_unary(e1, \"neg\"))     stop(sprintf(\"Unary op '%s' not supported for mlx\", op))   }   # Coerce scalars/matrix to mlx   if (!is.mlx(e1)) e1 <- as_mlx(e1)   if (!is.mlx(e2)) e2 <- as_mlx(e2)   if (op %in% c(\"+\",\"-\",\"*\",\"/\",\"^\")) return(.mlx_binary(e1, e2, op))   if (op %in% c(\"==\",\"!=\",\"<\",\"<=\",\">\",\">=\")) return(.mlx_binary(e1, e2, op))   stop(sprintf(\"Op '%s' not supported for mlx\", op)) } `%*%.mlx` <- function(x, y) {   if (!is.mlx(x)) x <- as_mlx(x)   if (!is.mlx(y)) y <- as_mlx(y)   new_mlx(cpp_mlx_matmul(x$ptr, y$ptr), c(x$dim[1L], y$dim[length(y$dim)]), promote_dtype(x,y), common_device(x,y)) } .onLoad <- function(...) {   # Ensure our method is visible for dispatch   # (S3 method for primitive is recognized if named `%*%.mlx`) }"},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-6--indexing-printing-diagnostics","dir":"","previous_headings":"","what":"Milestone 6 — Indexing, printing, diagnostics","title":"Project: Rmlx","text":"T20. Indexing [ ] R method: C++: SEXP cpp_mlx_slice(SEXP x, SEXP i_, SEXP j_); Support integer/real/logical vectors; negative indices throw (translate). Use MLX slicing ops (start/stop/step per axis). Acceptance: x[ ,1], x[1:5, 3:7], logical masks rows/cols (optional v1) behave; returns mlx. T21. Shape/dtype accessors R: mlx_dim <- function(x) x$dim mlx_dtype <- function(x) x$dtype dim.mlx <- function(x) x$dim (S3) length.mlx <- function(x) prod(x$dim) T22. Error messages Ensure C++ functions translate exceptions R errors actionable messages: incompatible shapes, dtype mismatch, found MLX op, etc.","code":"`[.mlx` <- function(x, i, j, ..., drop = TRUE) {   # Convert missing to full spans, build slices, call C++ slice   new_mlx(cpp_mlx_slice(x$ptr, normalize_index(i), normalize_index(j)), new_dim, x$dtype, x$device) }"},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-7--devicestream-management","dir":"","previous_headings":"","what":"Milestone 7 — Device/stream management","title":"Project: Rmlx","text":"T23. Default device R: mlx_default_device <- local({ dev <- \"gpu\"; function(value) { (!missing(value)) dev <<- match.arg(value, c(\"gpu\",\"cpu\")); dev }}) Used as_mlx() constructors device specified. C++: Keep references MLX CPU/GPU streams (e.g., MLX_GPU_STREAM, MLX_CPU_STREAM identifiers). Make small helper select stream per call. Acceptance: Users can switch default CPU debugging; ops route accordingly. T24. Synchronization R: mlx_synchronize(device=c(\"gpu\",\"cpu\")) → C++ call stream/device synchronize (MLX exposes ). Acceptance: outstanding work remains synchronize.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-8--docs-examples-tests","dir":"","previous_headings":"","what":"Milestone 8 — Docs, examples, tests","title":"Project: Rmlx","text":"T25. Roxygen2 docs Add #' docs user-facing functions: as_mlx, .matrix.mlx, mlx_eval, ops overview, %*%, colMeans.mlx, rowMeans.mlx, etc. ?mlx overview man page: explain laziness, evaluation points, device selection, unified memory concept. T26. Vignette vignettes/getting-started.Rmd Walkthrough: creating mlx arrays, arithmetic, %*%, reductions, colMeans, evaluation/convert, simple timing demo vs base R large matmul (note: formal benchmark). State Apple-requirement. T27. Tests (testthat) Skip tests MLX unavailable device GPU: Tests: Roundtrip as_mlx → .matrix equality. Elementwise ops vs base R (small sizes). %*% correctness vs base R. colMeans/rowMeans/sum/mean equality vs base R. Broadcasting cases. Indexing behavior. Tolerances: use expect_equal(..., tolerance = 1e-6).","code":"skip_if_not(mlx_available())"},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"milestone-9--build-local-check-packaging","dir":"","previous_headings":"","what":"Milestone 9 — Build, local check, packaging","title":"Project: Rmlx","text":"T28. Local build Commands: R CMD build . R CMD INSTALL Rmlx_0.0.0.9000.tar.gz R CMD check Rmlx_0.0.0.9000.tar.gz ---cran Accept: ERRORs; NOTE/WARN SystemRequirements acceptable. T29. Minimal examples Add examples function docs run fast touch small arrays avoid GPU timeouts.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"file-by-file-stubs-for-the-agent","dir":"","previous_headings":"","what":"File-by-file stubs (for the agent)","title":"Project: Rmlx","text":"R/class.R R/ops.R (skeleton shown earlier) R/stats.R src/Makevars (generated configure) src/registration.cpp src/mlx_bindings.cpp (sketch)","code":"#' Create MLX array from R object #' @export as_mlx <- function(x, dtype = c(\"float32\",\"float64\"), device = mlx_default_device()) {   dtype <- match.arg(dtype)   if (is.mlx(x)) return(x)   if (is.vector(x)) dim <- length(x) else dim <- dim(x)   stopifnot(!is.null(dim))   ptr <- cpp_mlx_from_numeric(as.numeric(x), as.integer(dim), dtype, device)   structure(list(ptr = ptr, dim = as.integer(dim), dtype = dtype, device = device), class = \"mlx\") }  #' Force evaluation #' @export mlx_eval <- function(x) { stopifnot(is.mlx(x)); cpp_mlx_eval(x$ptr); invisible(x) }  #' Convert MLX array to base matrix/array #' @export as.matrix.mlx <- function(x, ...) {   mlx_eval(x)   out <- cpp_mlx_to_numeric(x$ptr)   dim(out) <- x$dim   out }  is.mlx <- function(x) inherits(x, \"mlx\") #' @export sum.mlx  <- function(x, ...) .mlx_reduce(x, \"sum\") #' @export mean.mlx <- function(x, ...) .mlx_reduce(x, \"mean\")  #' @export colMeans.mlx <- function(x, na.rm = FALSE, dims = 1, ...) .mlx_reduce_axis(x, \"mean\", axis = 0L, keepdims = FALSE) #' @export rowMeans.mlx <- function(x, na.rm = FALSE, dims = 1, ...) .mlx_reduce_axis(x, \"mean\", axis = 1L, keepdims = FALSE)  #' @export t.mlx <- function(x) new_mlx(cpp_mlx_transpose(x$ptr), rev(x$dim), x$dtype, x$device)  #' @export crossprod.mlx <- function(x, y = NULL) { if (is.null(y)) y <- x; t(x) %*% y } #' @export tcrossprod.mlx <- function(x, y = NULL) { if (is.null(y)) y <- x; x %*% t(y) } PKG_CPPFLAGS = -I$(MLX_INCLUDE) PKG_LIBS     = -L$(MLX_LIB_DIR) -lmlx #include <Rcpp.h> using namespace Rcpp; // forward-declare C++ functions to register with R // RCPP_MODULE / R_registerRoutines as needed  extern \"C\" {   void R_init_Rmlx(DllInfo *dll) {     R_registerRoutines(dll, NULL, NULL, NULL, NULL);     R_useDynamicSymbols(dll, TRUE);   } } #include <Rcpp.h> #include \"mlx_bindings.hpp\" #include <mlx/c_api.h>  // adjust include path per installed MLX  using namespace Rcpp;  // Helpers to unwrap/wrap external pointers, set dims/dtype/device (store those in R side)  SEXP cpp_mlx_from_numeric(SEXP x_, SEXP dim_, SEXP dtype_, SEXP device_) {   NumericVector x(x_);   IntegerVector dim(dim_);   std::string dtype = as<std::string>(dtype_);   std::string device = as<std::string>(device_);    // create MLX array with given shape and dtype on device   // copy x.data() into MLX array   // return XPtr<MlxArray>(new MlxArray(ptr), true) }  SEXP cpp_mlx_to_numeric(SEXP xp_) {   // ensure evaluated   // copy MLX array data to NumericVector }  void cpp_mlx_eval(SEXP xp_) {   // call MLX eval on underlying array/graph }  // unary, binary, reduce, reduce_axis, transpose, reshape, matmul, slice..."},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"acceptance-checklist-phase-1-complete","dir":"","previous_headings":"","what":"Acceptance checklist (Phase 1 complete)","title":"Project: Rmlx","text":"Build succeeds Apple Silicon MLX installed. as_mlx(), .matrix.mlx() roundtrip correct. + - * / ^, comparisons, %*% produce correct results vs base R (small sizes). sum.mlx, mean.mlx, colMeans.mlx, rowMeans.mlx, t.mlx, crossprod.mlx, tcrossprod.mlx correct. Lazy default; .matrix.mlx() forces evaluation; mlx_eval() works. Indexing [] supports common cases. Helpful errors MLX found install, shape/dtype mismatches runtime. Vignette explains usage caveats.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"risks--notes-for-the-agent","dir":"","previous_headings":"","what":"Risks & notes for the agent","title":"Project: Rmlx","text":"MLX headers & symbols: Ensure correct include (e.g., #include <mlx/c_api.h>) link flags; actual header path & lib name may vary; keep configure flexible env overrides. Dtype: R double default; ’ll allow float32 float64. Decide default (float32 faster GPU; match R expectations, maybe default float64; document choice). Axis conventions: Confirm MLX axis numbering vs R’s column-major expectations; lock tests colMeans/rowMeans. Broadcasting: Implement consistent rules; add tests scalar + array, vector + matrix. Evaluation semantics: MLX needs explicit graph roots streams eval, store whatever handle required array (per-session singleton), make cpp_mlx_eval robust. Thread safety: Rcpp calls execute R main thread; ensure MLX usage safe context. Indexing: Logical indexing may deferred; start integer ranges : slices.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/plan.html","id":"phase-2-later-not-in-scope-for-this-handoff","dir":"","previous_headings":"","what":"Phase 2 (later, not in scope for this handoff)","title":"Project: Rmlx","text":"Autodiff: wrap MLX grad transforms; expose mlx_grad(fn, params) R closures provide graph-based differentiation APIs. Optimizers: SGD/Adam mlx parameters. linalg: solve, chol, svd, eigen (depending MLX support). Datasets/dataloaders, random seeding, fused kernels, compilation.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Math.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Math operations for MLX arrays — Math.mlx","title":"Math operations for MLX arrays — Math.mlx","text":"Math operations MLX arrays","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Math.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Math operations for MLX arrays — Math.mlx","text":"","code":"# S3 method for class 'mlx' Math(x, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/Math.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Math operations for MLX arrays — Math.mlx","text":"x mlx object ... Additional arguments (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Math.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Math operations for MLX arrays — Math.mlx","text":"mlx object result","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/Math.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Math operations for MLX arrays — Math.mlx","text":"","code":"x <- as_mlx(matrix(c(-1, 0, 1), 3, 1)) sin(x) #> mlx array [3 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>           [,1] #> [1,] -0.841471 #> [2,]  0.000000 #> [3,]  0.841471 round(x + 0.4) #> mlx array [3 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] #> [1,]   -1 #> [2,]    0 #> [3,]    1"},{"path":"https://hughjonesd.github.io/Rmlx/reference/Ops.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Arithmetic and comparison operators for MLX arrays — Ops.mlx","title":"Arithmetic and comparison operators for MLX arrays — Ops.mlx","text":"Arithmetic comparison operators MLX arrays","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Ops.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Arithmetic and comparison operators for MLX arrays — Ops.mlx","text":"","code":"# S3 method for class 'mlx' Ops(e1, e2 = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/Ops.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Arithmetic and comparison operators for MLX arrays — Ops.mlx","text":"e1 First operand (mlx numeric) e2 Second operand (mlx numeric)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Ops.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Arithmetic and comparison operators for MLX arrays — Ops.mlx","text":"mlx object","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/Ops.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Arithmetic and comparison operators for MLX arrays — Ops.mlx","text":"","code":"if (FALSE) { # \\dontrun{ x <- as_mlx(matrix(1:4, 2, 2)) y <- as_mlx(matrix(5:8, 2, 2)) x + y x < y } # }"},{"path":"https://hughjonesd.github.io/Rmlx/reference/Rmlx-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","title":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","text":"package provides R interface Apple's MLX (Machine Learning eXchange) library GPU-accelerated array operations Apple Silicon.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Rmlx-package.html","id":"key-features","dir":"Reference","previous_headings":"","what":"Key Features","title":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","text":"Lazy evaluation: Operations computed explicitly evaluated GPU acceleration: Leverage Metal Apple Silicon Familiar syntax: S3 methods standard R operations Unified memory: Efficient data sharing CPU GPU","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Rmlx-package.html","id":"main-functions","dir":"Reference","previous_headings":"","what":"Main Functions","title":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","text":"as_mlx(): Convert R objects MLX arrays .matrix.mlx(): Convert MLX arrays back R mlx_eval(): Force evaluation lazy operations Arithmetic: +, -, *, /, ^ Matrix ops: %*%, t, crossprod, tcrossprod Reductions: sum, mean, colMeans, rowMeans","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Rmlx-package.html","id":"lazy-evaluation","dir":"Reference","previous_headings":"","what":"Lazy Evaluation","title":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","text":"MLX arrays use lazy evaluation default. Operations recorded executed : call mlx_eval(x) convert R .matrix(x) result needed another computation","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/Rmlx-package.html","id":"device-management","dir":"Reference","previous_headings":"","what":"Device Management","title":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","text":"Use mlx_default_device() control whether arrays created GPU (default) CPU. mlx arrays stored float32 regardless device. Use base R arrays require float64 math.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/Rmlx-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Rmlx: R Interface to Apple's MLX Arrays — Rmlx-package","text":"Maintainer: David Hugh-Jones david@hughjones.com","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.array.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert MLX array to R array — as.array.mlx","title":"Convert MLX array to R array — as.array.mlx","text":"Convert MLX array R array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.array.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert MLX array to R array — as.array.mlx","text":"","code":"# S3 method for class 'mlx' as.array(x, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.array.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert MLX array to R array — as.array.mlx","text":"x mlx object ... Additional arguments (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.array.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert MLX array to R array — as.array.mlx","text":"numeric array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.array.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert MLX array to R array — as.array.mlx","text":"","code":"x <- as_mlx(matrix(1:8, 2, 4)) as.array(x) #>      [,1] [,2] [,3] [,4] #> [1,]    1    3    5    7 #> [2,]    2    4    6    8"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.matrix.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert MLX array to R matrix/array — as.matrix.mlx","title":"Convert MLX array to R matrix/array — as.matrix.mlx","text":"Convert MLX array R matrix/array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.matrix.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert MLX array to R matrix/array — as.matrix.mlx","text":"","code":"# S3 method for class 'mlx' as.matrix(x, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.matrix.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert MLX array to R matrix/array — as.matrix.mlx","text":"x mlx object ... Additional arguments (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.matrix.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert MLX array to R matrix/array — as.matrix.mlx","text":"matrix array (numeric logical depending dtype)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.matrix.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert MLX array to R matrix/array — as.matrix.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) as.matrix(x) #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.vector.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert MLX array to R vector — as.vector.mlx","title":"Convert MLX array to R vector — as.vector.mlx","text":"Convert MLX array R vector","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.vector.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert MLX array to R vector — as.vector.mlx","text":"","code":"# S3 method for class 'mlx' as.vector(x, mode = \"any\")"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.vector.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert MLX array to R vector — as.vector.mlx","text":"x mlx object mode Character string specifying mode (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.vector.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert MLX array to R vector — as.vector.mlx","text":"numeric vector","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as.vector.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert MLX array to R vector — as.vector.mlx","text":"","code":"x <- as_mlx(1:5) as.vector(x) #> [1] 1 2 3 4 5"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as_mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Create MLX array from R object — as_mlx","title":"Create MLX array from R object — as_mlx","text":"Create MLX array R object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as_mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create MLX array from R object — as_mlx","text":"","code":"as_mlx(   x,   dtype = c(\"float32\", \"float64\", \"bool\", \"complex64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/as_mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create MLX array from R object — as_mlx","text":"x Numeric, logical, complex vector, matrix, array convert dtype Ignored. Present backward compatibility. Numeric arrays stored float32; logical arrays use MLX bool; complex arrays use MLX complex64. device Device: \"gpu\" (default) \"cpu\"","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as_mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create MLX array from R object — as_mlx","text":"object class mlx","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/as_mlx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create MLX array from R object — as_mlx","text":"Apple MLX executes single precision. Numeric inputs stored float32 regardless requested dtype. Logical inputs mapped MLX boolean tensors. Complex inputs stored complex64 (float32 real imaginary parts). Asking dtype = \"float64\" emits warning input downcast float32. require double precision arithmetic, use base R arrays instead mlx objects.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/as_mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create MLX array from R object — as_mlx","text":"","code":"x <- as_mlx(matrix(1:12, 3, 4))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/cbind.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Column-bind MLX tensors — cbind.mlx","title":"Column-bind MLX tensors — cbind.mlx","text":"Column-bind MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/cbind.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Column-bind MLX tensors — cbind.mlx","text":"","code":"# S3 method for class 'mlx' cbind(..., deparse.level = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/cbind.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Column-bind MLX tensors — cbind.mlx","text":"... Objects bind. MLX tensors kept MLX; inputs coerced via as_mlx(). deparse.level Compatibility argument accepted S3 dispatch; ignored.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/cbind.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Column-bind MLX tensors — cbind.mlx","text":"mlx tensor stacked along second axis.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/cbind.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Column-bind MLX tensors — cbind.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) y <- as_mlx(matrix(5:8, 2, 2)) cbind(x, y) #> mlx array [2 x 4] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] [,4] #> [1,]    1    3    5    7 #> [2,]    2    4    6    8"},{"path":"https://hughjonesd.github.io/Rmlx/reference/chol.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Cholesky decomposition for MLX tensors — chol.mlx","title":"Cholesky decomposition for MLX tensors — chol.mlx","text":"Cholesky decomposition MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/chol.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cholesky decomposition for MLX tensors — chol.mlx","text":"","code":"# S3 method for class 'mlx' chol(x, pivot = FALSE, LINPACK = FALSE, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/chol.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cholesky decomposition for MLX tensors — chol.mlx","text":"x mlx positive-definite matrix. pivot Ignored; pivoted decomposition supported. LINPACK Ignored; set FALSE. ... Additional arguments (unused).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/chol.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cholesky decomposition for MLX tensors — chol.mlx","text":"Upper-triangular Cholesky factor mlx matrix.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/chol.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cholesky decomposition for MLX tensors — chol.mlx","text":"","code":"x <- as_mlx(matrix(c(4, 1, 1, 3), 2, 2)) chol(x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1]     [,2] #> [1,]    2 0.500000 #> [2,]    0 1.658312"},{"path":"https://hughjonesd.github.io/Rmlx/reference/colMeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Column means for MLX tensors — colMeans","title":"Column means for MLX tensors — colMeans","text":"Column means MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/colMeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Column means for MLX tensors — colMeans","text":"","code":"colMeans(x, na.rm = FALSE, dims = 1, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/colMeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Column means for MLX tensors — colMeans","text":"x array mlx tensor. na.rm Logical; currently ignored mlx tensors. dims Dimensions passed base implementation x mlx tensor. ... Additional arguments forwarded base implementation.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/colMeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Column means for MLX tensors — colMeans","text":"mlx tensor x mlx, otherwise numeric vector.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/colMeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Column means for MLX tensors — colMeans","text":"","code":"x <- as_mlx(matrix(1:6, 3, 2)) colMeans(x) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 2 5"},{"path":"https://hughjonesd.github.io/Rmlx/reference/colSums.html","id":null,"dir":"Reference","previous_headings":"","what":"Column sums for MLX tensors — colSums","title":"Column sums for MLX tensors — colSums","text":"Column sums MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/colSums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Column sums for MLX tensors — colSums","text":"","code":"colSums(x, na.rm = FALSE, dims = 1, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/colSums.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Column sums for MLX tensors — colSums","text":"x array mlx tensor. na.rm Logical; currently ignored mlx tensors. dims Dimensions passed base implementation x mlx tensor. ... Additional arguments forwarded base implementation.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/colSums.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Column sums for MLX tensors — colSums","text":"mlx tensor x mlx, otherwise numeric vector.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/colSums.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Column sums for MLX tensors — colSums","text":"","code":"x <- as_mlx(matrix(1:6, 3, 2)) colSums(x) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1]  6 15"},{"path":"https://hughjonesd.github.io/Rmlx/reference/crossprod.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross product — crossprod.mlx","title":"Cross product — crossprod.mlx","text":"Cross product","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/crossprod.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross product — crossprod.mlx","text":"","code":"# S3 method for class 'mlx' crossprod(x, y = NULL, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/crossprod.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross product — crossprod.mlx","text":"x mlx matrix y mlx matrix (default: NULL, uses x) ... Additional arguments passed base::crossprod.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/crossprod.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross product — crossprod.mlx","text":"t(x) %*% y mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/crossprod.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross product — crossprod.mlx","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) crossprod(x) #> mlx array [3 x 3] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] #> [1,]    5   11   17 #> [2,]   11   25   39 #> [3,]   17   39   61"},{"path":"https://hughjonesd.github.io/Rmlx/reference/dim.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dimensions of MLX array — dim.mlx","title":"Get dimensions of MLX array — dim.mlx","text":"Get dimensions MLX array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/dim.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dimensions of MLX array — dim.mlx","text":"","code":"# S3 method for class 'mlx' dim(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/dim.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dimensions of MLX array — dim.mlx","text":"x mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/dim.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dimensions of MLX array — dim.mlx","text":"Integer vector dimensions","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/dim.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get dimensions of MLX array — dim.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) dim(x) #> [1] 2 2"},{"path":"https://hughjonesd.github.io/Rmlx/reference/fft.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Fourier Transform — fft","title":"Fast Fourier Transform — fft","text":"Extends stats::fft() work mlx objects delegating standard R implementation inputs.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/fft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Fourier Transform — fft","text":"","code":"fft(z, inverse = FALSE, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/fft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Fourier Transform — fft","text":"z Input transform. May numeric, complex, mlx object. inverse Logical flag; TRUE compute inverse transform. ... Passed default method.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/fft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast Fourier Transform — fft","text":"mlx inputs, mlx object containing complex frequency coefficients; otherwise base R result.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/fft.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Fourier Transform — fft","text":"","code":"z <- as_mlx(c(1, 2, 3, 4)) fft(z) #> mlx array [4] #>   dtype: complex64 #>   device: gpu #>   values: #> [1] 10+0i -2+2i -2+0i -2-2i fft(z, inverse = TRUE) #> mlx array [4] #>   dtype: complex64 #>   device: gpu #>   values: #> [1] 10+0i -2-2i -2+0i -2+2i"},{"path":"https://hughjonesd.github.io/Rmlx/reference/grapes-times-grapes-.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix multiplication for MLX arrays — %*%.mlx","title":"Matrix multiplication for MLX arrays — %*%.mlx","text":"Matrix multiplication MLX arrays","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/grapes-times-grapes-.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix multiplication for MLX arrays — %*%.mlx","text":"","code":"# S3 method for class 'mlx' x %*% y"},{"path":"https://hughjonesd.github.io/Rmlx/reference/grapes-times-grapes-.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix multiplication for MLX arrays — %*%.mlx","text":"x, y numeric complex matrices vectors.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/grapes-times-grapes-.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix multiplication for MLX arrays — %*%.mlx","text":"mlx object","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/grapes-times-grapes-.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix multiplication for MLX arrays — %*%.mlx","text":"","code":"if (FALSE) { # \\dontrun{ x <- as_mlx(matrix(1:6, 2, 3)) y <- as_mlx(matrix(1:6, 3, 2)) x %*% y } # }"},{"path":"https://hughjonesd.github.io/Rmlx/reference/is.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is an MLX array — is.mlx","title":"Test if object is an MLX array — is.mlx","text":"Test object MLX array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/is.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is an MLX array — is.mlx","text":"","code":"is.mlx(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/is.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is an MLX array — is.mlx","text":"x Object test","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/is.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is an MLX array — is.mlx","text":"Logical","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/is.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test if object is an MLX array — is.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) is.mlx(x) #> [1] TRUE"},{"path":"https://hughjonesd.github.io/Rmlx/reference/length.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Get length of MLX array — length.mlx","title":"Get length of MLX array — length.mlx","text":"Get length MLX array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/length.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get length of MLX array — length.mlx","text":"","code":"# S3 method for class 'mlx' length(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/length.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get length of MLX array — length.mlx","text":"x mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/length.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get length of MLX array — length.mlx","text":"Total number elements","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/length.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get length of MLX array — length.mlx","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) length(x) #> [1] 6"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mean.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean of MLX array elements — mean.mlx","title":"Mean of MLX array elements — mean.mlx","text":"Mean MLX array elements","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mean.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean of MLX array elements — mean.mlx","text":"","code":"# S3 method for class 'mlx' mean(x, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mean.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean of MLX array elements — mean.mlx","text":"x mlx object ... Additional arguments (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mean.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean of MLX array elements — mean.mlx","text":"mlx scalar","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mean.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mean of MLX array elements — mean.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mean(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 2.5"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_arange.html","id":null,"dir":"Reference","previous_headings":"","what":"Numerical ranges on MLX devices — mlx_arange","title":"Numerical ranges on MLX devices — mlx_arange","text":"mlx_arange() mirrors base::seq() MLX tensors: creates evenly spaced values starting start (default 0), stepping step (default 1), stopping stop.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_arange.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Numerical ranges on MLX devices — mlx_arange","text":"","code":"mlx_arange(   stop,   start = NULL,   step = NULL,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_arange.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Numerical ranges on MLX devices — mlx_arange","text":"stop Exclusive upper bound. start Optional starting value (defaults 0). step Optional step size (defaults 1). dtype MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_arange.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Numerical ranges on MLX devices — mlx_arange","text":"1D mlx tensor.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_arange.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Numerical ranges on MLX devices — mlx_arange","text":"","code":"mlx_arange(5)                    # 0, 1, 2, 3, 4 #> mlx array [5] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 0 1 2 3 4 mlx_arange(5, start = 1, step = 2) # 1, 3 #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1 3"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_argmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Argmax and argmin on MLX tensors — mlx_argmax","title":"Argmax and argmin on MLX tensors — mlx_argmax","text":"Argmax argmin MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_argmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Argmax and argmin on MLX tensors — mlx_argmax","text":"","code":"mlx_argmax(x, axis = NULL, keepdims = FALSE)  mlx_argmin(x, axis = NULL, keepdims = FALSE)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_argmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Argmax and argmin on MLX tensors — mlx_argmax","text":"x object coercible mlx. axis Optional axis operate (1-indexed like R). NULL, tensor flattened first. keepdims Logical; retain reduced dimensions length one.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_argmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Argmax and argmin on MLX tensors — mlx_argmax","text":"mlx tensor indices.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_argmax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Argmax and argmin on MLX tensors — mlx_argmax","text":"","code":"x <- as_mlx(matrix(c(1, 5, 3, 2), 2, 2)) mlx_argmax(x) #> mlx array [] #>   dtype: uint32 #>   device: gpu #>   values: #> [1] 2 mlx_argmax(x, axis = 1) #> mlx array [2] #>   dtype: uint32 #>   device: gpu #>   values: #> [1] 1 0 mlx_argmin(x) #> mlx array [] #>   dtype: uint32 #>   device: gpu #>   values: #> [1] 0"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_array_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Roxygen parameters for arrays — mlx_array_params","title":"Roxygen parameters for arrays — mlx_array_params","text":"Roxygen parameters arrays","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_array_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Roxygen parameters for arrays — mlx_array_params","text":"x mlx array object coercible mlx (varies function). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_clip.html","id":null,"dir":"Reference","previous_headings":"","what":"Clip MLX tensor values into a range — mlx_clip","title":"Clip MLX tensor values into a range — mlx_clip","text":"Clip MLX tensor values range","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_clip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clip MLX tensor values into a range — mlx_clip","text":"","code":"mlx_clip(x, min = NULL, max = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_clip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clip MLX tensor values into a range — mlx_clip","text":"x mlx tensor coercible object. min, max Scalar bounds. Use NULL leave bound open.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_clip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clip MLX tensor values into a range — mlx_clip","text":"mlx tensor values clipped [min, max].","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_clip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clip MLX tensor values into a range — mlx_clip","text":"","code":"if (FALSE) { # \\dontrun{ x <- as_mlx(rnorm(4)) mlx_clip(x, min = -1, max = 1) } # }"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_creation_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Roxygen parameters for mlx creation — mlx_creation_params","title":"Roxygen parameters for mlx creation — mlx_creation_params","text":"Roxygen parameters mlx creation","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_creation_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Roxygen parameters for mlx creation — mlx_creation_params","text":"dim Integer vector giving tensor shape. dtype MLX dtype. Supported types vary function (see Details). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_cross.html","id":null,"dir":"Reference","previous_headings":"","what":"Vector cross product with MLX tensors — mlx_cross","title":"Vector cross product with MLX tensors — mlx_cross","text":"Vector cross product MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_cross.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vector cross product with MLX tensors — mlx_cross","text":"","code":"mlx_cross(a, b, axis = -1L)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_cross.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vector cross product with MLX tensors — mlx_cross","text":", b Input mlx tensors containing 3D vectors. axis Axis along compute cross product (1-indexed, default last).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_cross.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vector cross product with MLX tensors — mlx_cross","text":"mlx tensor cross products.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_cross.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vector cross product with MLX tensors — mlx_cross","text":"","code":"u <- as_mlx(c(1, 0, 0)) v <- as_mlx(c(0, 1, 0)) mlx_cross(u, v) #> mlx array [3] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 0 0 1"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_default_device.html","id":null,"dir":"Reference","previous_headings":"","what":"Get or set default MLX device — mlx_default_device","title":"Get or set default MLX device — mlx_default_device","text":"Get set default MLX device","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_default_device.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get or set default MLX device — mlx_default_device","text":"","code":"mlx_default_device(value)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_default_device.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get or set default MLX device — mlx_default_device","text":"value New default device (\"gpu\" \"cpu\"). missing, returns current default.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_default_device.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get or set default MLX device — mlx_default_device","text":"Current default device (character)","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_default_device.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get or set default MLX device — mlx_default_device","text":"","code":"mlx_default_device()  # Get current default #> [1] \"gpu\" mlx_default_device(\"cpu\")  # Set to CPU #> [1] \"cpu\" mlx_default_device(\"gpu\")  # Set back to GPU #> [1] \"gpu\" mlx_default_device() #> [1] \"gpu\""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dim.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dimensions helper — mlx_dim","title":"Get dimensions helper — mlx_dim","text":"Get dimensions helper","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dimensions helper — mlx_dim","text":"","code":"mlx_dim(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dimensions helper — mlx_dim","text":"x mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dimensions helper — mlx_dim","text":"Dimensions","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get dimensions helper — mlx_dim","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) mlx_dim(x) #> [1] 2 3"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dtype.html","id":null,"dir":"Reference","previous_headings":"","what":"Get data type helper — mlx_dtype","title":"Get data type helper — mlx_dtype","text":"Get data type helper","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dtype.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get data type helper — mlx_dtype","text":"","code":"mlx_dtype(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dtype.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get data type helper — mlx_dtype","text":"x mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dtype.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get data type helper — mlx_dtype","text":"Data type string","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_dtype.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get data type helper — mlx_dtype","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) mlx_dtype(x) #> [1] \"float32\""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eig.html","id":null,"dir":"Reference","previous_headings":"","what":"Eigen decomposition for MLX tensors — mlx_eig","title":"Eigen decomposition for MLX tensors — mlx_eig","text":"Eigen decomposition MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eig.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eigen decomposition for MLX tensors — mlx_eig","text":"","code":"mlx_eig(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eig.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Eigen decomposition for MLX tensors — mlx_eig","text":"x mlx square matrix.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eig.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Eigen decomposition for MLX tensors — mlx_eig","text":"list components values vectors, mlx tensors.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eig.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Eigen decomposition for MLX tensors — mlx_eig","text":"","code":"x <- as_mlx(matrix(c(2, -1, 0, 2), 2, 2)) eig <- mlx_eig(x) eig$values #> mlx array [2] #>   dtype: complex64 #>   device: gpu #>   values: #> [1] 2+0i 2+0i eig$vectors #> mlx array [2 x 2] #>   dtype: complex64 #>   device: gpu #>   values: #>                 [,1] [,2] #> [1,] 2.384186e-07+0i 0+0i #> [2,] 1.000000e+00+0i 1+0i"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigh.html","id":null,"dir":"Reference","previous_headings":"","what":"Eigen decomposition of Hermitian MLX tensors — mlx_eigh","title":"Eigen decomposition of Hermitian MLX tensors — mlx_eigh","text":"Eigen decomposition Hermitian MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eigen decomposition of Hermitian MLX tensors — mlx_eigh","text":"","code":"mlx_eigh(x, uplo = c(\"L\", \"U\"))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Eigen decomposition of Hermitian MLX tensors — mlx_eigh","text":"x mlx square matrix. uplo Character string indicating triangle use (\"L\" \"U\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Eigen decomposition of Hermitian MLX tensors — mlx_eigh","text":"list components values vectors.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Eigen decomposition of Hermitian MLX tensors — mlx_eigh","text":"","code":"x <- as_mlx(matrix(c(2, 1, 1, 3), 2, 2)) mlx_eigh(x) #> $values #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1.381966 3.618034 #>  #> $vectors #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>            [,1]      [,2] #> [1,] -0.8506508 0.5257311 #> [2,]  0.5257311 0.8506508 #>"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvals.html","id":null,"dir":"Reference","previous_headings":"","what":"Eigenvalues of MLX tensors — mlx_eigvals","title":"Eigenvalues of MLX tensors — mlx_eigvals","text":"Eigenvalues MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eigenvalues of MLX tensors — mlx_eigvals","text":"","code":"mlx_eigvals(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Eigenvalues of MLX tensors — mlx_eigvals","text":"x mlx square matrix.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Eigenvalues of MLX tensors — mlx_eigvals","text":"mlx tensor containing eigenvalues.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Eigenvalues of MLX tensors — mlx_eigvals","text":"","code":"x <- as_mlx(matrix(c(3, 1, 0, 2), 2, 2)) mlx_eigvals(x) #> mlx array [2] #>   dtype: complex64 #>   device: gpu #>   values: #> [1] 3+0i 2+0i"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvalsh.html","id":null,"dir":"Reference","previous_headings":"","what":"Eigenvalues of Hermitian MLX tensors — mlx_eigvalsh","title":"Eigenvalues of Hermitian MLX tensors — mlx_eigvalsh","text":"Eigenvalues Hermitian MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvalsh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eigenvalues of Hermitian MLX tensors — mlx_eigvalsh","text":"","code":"mlx_eigvalsh(x, uplo = c(\"L\", \"U\"))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvalsh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Eigenvalues of Hermitian MLX tensors — mlx_eigvalsh","text":"x mlx square matrix. uplo Character string indicating triangle use (\"L\" \"U\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvalsh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Eigenvalues of Hermitian MLX tensors — mlx_eigvalsh","text":"mlx tensor containing eigenvalues.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eigvalsh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Eigenvalues of Hermitian MLX tensors — mlx_eigvalsh","text":"","code":"x <- as_mlx(matrix(c(2, 1, 1, 3), 2, 2)) mlx_eigvalsh(x) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1.381966 3.618034"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eval.html","id":null,"dir":"Reference","previous_headings":"","what":"Force evaluation of lazy MLX operations — mlx_eval","title":"Force evaluation of lazy MLX operations — mlx_eval","text":"Force evaluation lazy MLX operations","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Force evaluation of lazy MLX operations — mlx_eval","text":"","code":"mlx_eval(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Force evaluation of lazy MLX operations — mlx_eval","text":"x mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Force evaluation of lazy MLX operations — mlx_eval","text":"input object (invisibly)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Force evaluation of lazy MLX operations — mlx_eval","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_eval(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_expand_dims.html","id":null,"dir":"Reference","previous_headings":"","what":"Insert singleton dimensions — mlx_expand_dims","title":"Insert singleton dimensions — mlx_expand_dims","text":"Insert singleton dimensions","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_expand_dims.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insert singleton dimensions — mlx_expand_dims","text":"","code":"mlx_expand_dims(x, axis)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_expand_dims.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insert singleton dimensions — mlx_expand_dims","text":"x mlx tensor. axis Integer vector axis positions (1-indexed) new singleton dimensions inserted.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_expand_dims.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insert singleton dimensions — mlx_expand_dims","text":"mlx tensor additional dimensions length one.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_expand_dims.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Insert singleton dimensions — mlx_expand_dims","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_expand_dims(x, axis = 1) #> mlx array [1 x 2 x 2] #>   dtype: float32 #>   device: gpu #>   (4 elements, not shown)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eye.html","id":null,"dir":"Reference","previous_headings":"","what":"Identity-like matrices on MLX devices — mlx_eye","title":"Identity-like matrices on MLX devices — mlx_eye","text":"Identity-like matrices MLX devices","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eye.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identity-like matrices on MLX devices — mlx_eye","text":"","code":"mlx_eye(   n,   m = n,   k = 0L,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eye.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identity-like matrices on MLX devices — mlx_eye","text":"n Number rows. m Optional number columns (defaults n). k Diagonal index: 0 main diagonal, positive values shift upward, negative values shift downward. dtype MLX dtype use (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eye.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identity-like matrices on MLX devices — mlx_eye","text":"mlx matrix ones selected diagonal zeros elsewhere.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_eye.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identity-like matrices on MLX devices — mlx_eye","text":"","code":"eye <- mlx_eye(3) upper_eye <- mlx_eye(3, k = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_forward.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward pass utility — mlx_forward","title":"Forward pass utility — mlx_forward","text":"Forward pass utility","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_forward.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward pass utility — mlx_forward","text":"","code":"mlx_forward(module, x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_forward.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward pass utility — mlx_forward","text":"module mlx_module. x Input tensor.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_forward.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forward pass utility — mlx_forward","text":"Output tensor.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_forward.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forward pass utility — mlx_forward","text":"","code":"set.seed(1) layer <- mlx_linear(2, 1) input <- as_mlx(matrix(c(1, 2), 1, 2)) mlx_forward(layer, input) #> mlx array [1 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>            [,1] #> [1,] -0.2591672"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_full.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill an MLX tensor with a constant value — mlx_full","title":"Fill an MLX tensor with a constant value — mlx_full","text":"Fill MLX tensor constant value","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_full.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill an MLX tensor with a constant value — mlx_full","text":"","code":"mlx_full(dim, value, dtype = NULL, device = mlx_default_device())"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_full.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill an MLX tensor with a constant value — mlx_full","text":"dim Integer vector giving tensor shape. value Scalar value used fill tensor. Numeric, logical, complex. dtype MLX dtype (\"float32\", \"float64\", \"bool\", \"complex64\"). omitted, defaults \"complex64\" complex scalars, \"bool\" logical scalars, \"float32\" otherwise. device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_full.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill an MLX tensor with a constant value — mlx_full","text":"mlx tensor filled supplied value.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_full.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fill an MLX tensor with a constant value — mlx_full","text":"","code":"filled <- mlx_full(c(2, 2), 3.14) complex_full <- mlx_full(c(2, 2), 1+2i, dtype = \"complex64\")"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_grad.html","id":null,"dir":"Reference","previous_headings":"","what":"Automatic differentiation for MLX functions — mlx_grad","title":"Automatic differentiation for MLX functions — mlx_grad","text":"mlx_grad() computes gradients R function operates mlx tensors. function must keep differentiable computations MLX (e.g., via as_mlx() MLX operators) return mlx object.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_grad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Automatic differentiation for MLX functions — mlx_grad","text":"","code":"mlx_grad(f, ..., argnums = NULL, value = FALSE)  mlx_value_grad(f, ..., argnums = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_grad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Automatic differentiation for MLX functions — mlx_grad","text":"f R function. arguments mlx objects, return value must mlx tensor (typically scalar loss). ... Arguments pass f. coerced mlx needed. argnums Indices (1-based) identifying arguments differentiate respect . Defaults arguments. value function value returned alongside gradients? Set TRUE receive list components value grads.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_grad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Automatic differentiation for MLX functions — mlx_grad","text":"value = FALSE (default), list mlx tensors containing gradients order argnums. value = TRUE, list elements value (function output mlx) grads.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_grad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Automatic differentiation for MLX functions — mlx_grad","text":"Keep differentiated closure inside MLX operations. Coercing tensors back base R objects (.matrix(), .numeric(), [[ extraction) breaks gradient tape results error.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_grad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Automatic differentiation for MLX functions — mlx_grad","text":"","code":"loss <- function(w, x, y) {   preds <- x %*% w   resids <- preds - y   sum(resids * resids) / length(y) } x <- as_mlx(matrix(1:8, 4, 2)) y <- as_mlx(matrix(c(1, 3, 2, 4), 4, 1)) w <- as_mlx(matrix(0, 2, 1)) mlx_grad(loss, w, x, y)[[1]] #> mlx array [2 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>       [,1] #> [1,] -14.5 #> [2,] -34.5 loss <- function(w, x) sum((x %*% w) * (x %*% w)) x <- as_mlx(matrix(1:4, 2, 2)) w <- as_mlx(matrix(c(1, -1), 2, 1)) mlx_value_grad(loss, w, x) #> $value #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 8 #>  #> $grads #> $grads[[1]] #> mlx array [2 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] #> [1,]  -12 #> [2,]  -28 #>  #> $grads[[2]] #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]   -4    4 #> [2,]   -4    4 #>  #>"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_identity.html","id":null,"dir":"Reference","previous_headings":"","what":"Identity matrices on MLX devices — mlx_identity","title":"Identity matrices on MLX devices — mlx_identity","text":"Identity matrices MLX devices","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_identity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identity matrices on MLX devices — mlx_identity","text":"","code":"mlx_identity(n, dtype = c(\"float32\", \"float64\"), device = mlx_default_device())"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_identity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identity matrices on MLX devices — mlx_identity","text":"n Size square matrix. dtype MLX dtype use (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_identity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identity matrices on MLX devices — mlx_identity","text":"mlx identity matrix.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_identity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identity matrices on MLX devices — mlx_identity","text":"","code":"I4 <- mlx_identity(4)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a learnable linear transformation — mlx_linear","title":"Create a learnable linear transformation — mlx_linear","text":"Create learnable linear transformation","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a learnable linear transformation — mlx_linear","text":"","code":"mlx_linear(   in_features,   out_features,   bias = TRUE,   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a learnable linear transformation — mlx_linear","text":"in_features Number input features. out_features Number output features. bias bias term included? device Device parameters.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a learnable linear transformation — mlx_linear","text":"object class mlx_module.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a learnable linear transformation — mlx_linear","text":"","code":"set.seed(1) layer <- mlx_linear(3, 2) x <- as_mlx(matrix(1:6, 2, 3)) mlx_forward(layer, x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>           [,1]       [,2] #> [1,] -3.473105 -1.2398810 #> [2,] -4.516946 -0.3382074"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Evenly spaced ranges on MLX devices — mlx_linspace","title":"Evenly spaced ranges on MLX devices — mlx_linspace","text":"mlx_linspace() creates num evenly spaced values start stop, inclusive. Unlike mlx_arange(), specify many samples want rather step size.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evenly spaced ranges on MLX devices — mlx_linspace","text":"","code":"mlx_linspace(   start,   stop,   num = 50L,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evenly spaced ranges on MLX devices — mlx_linspace","text":"start Starting value. stop Final value (inclusive). num Number samples generate. dtype MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evenly spaced ranges on MLX devices — mlx_linspace","text":"1D mlx tensor.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_linspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evenly spaced ranges on MLX devices — mlx_linspace","text":"","code":"mlx_linspace(0, 1, num = 5) #> mlx array [5] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 0.00 0.25 0.50 0.75 1.00"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logcumsumexp.html","id":null,"dir":"Reference","previous_headings":"","what":"Log cumulative sum exponential for MLX tensors — mlx_logcumsumexp","title":"Log cumulative sum exponential for MLX tensors — mlx_logcumsumexp","text":"Log cumulative sum exponential MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logcumsumexp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log cumulative sum exponential for MLX tensors — mlx_logcumsumexp","text":"","code":"mlx_logcumsumexp(x, axis = NULL, reverse = FALSE, inclusive = TRUE)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logcumsumexp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log cumulative sum exponential for MLX tensors — mlx_logcumsumexp","text":"x object coercible mlx. axis Optional axis (single integer) operate . reverse Logical flag reverse accumulation. inclusive Logical flag controlling inclusivity.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logcumsumexp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log cumulative sum exponential for MLX tensors — mlx_logcumsumexp","text":"mlx tensor.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logcumsumexp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log cumulative sum exponential for MLX tensors — mlx_logcumsumexp","text":"","code":"x <- as_mlx(1:4) as.vector(as.matrix(mlx_logcumsumexp(x))) #> [1] 1.000000 2.313262 3.407606 4.440190 m <- as_mlx(matrix(1:6, 2, 3)) as.matrix(mlx_logcumsumexp(m, axis = 2)) #>      [,1]     [,2]     [,3] #> [1,]    1 3.126928 5.142931 #> [2,]    2 4.126928 6.142931"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logsumexp.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-sum-exp reduction for MLX tensors — mlx_logsumexp","title":"Log-sum-exp reduction for MLX tensors — mlx_logsumexp","text":"Log-sum-exp reduction MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logsumexp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-sum-exp reduction for MLX tensors — mlx_logsumexp","text":"","code":"mlx_logsumexp(x, axis = NULL, keepdims = FALSE)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logsumexp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-sum-exp reduction for MLX tensors — mlx_logsumexp","text":"x object coercible mlx. axis Optional axis operate (1-indexed like R). NULL, tensor flattened first. keepdims Logical indicating whether reduced axes retained.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logsumexp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-sum-exp reduction for MLX tensors — mlx_logsumexp","text":"mlx tensor containing log-sum-exp results.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_logsumexp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log-sum-exp reduction for MLX tensors — mlx_logsumexp","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) as.matrix(mlx_logsumexp(x)) #> [1] 6.456193 as.matrix(mlx_logsumexp(x, axis = 2)) #> [1] 5.142931 6.142931"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_maximum.html","id":null,"dir":"Reference","previous_headings":"","what":"Elementwise maximum of two MLX tensors — mlx_maximum","title":"Elementwise maximum of two MLX tensors — mlx_maximum","text":"Elementwise maximum two MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_maximum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Elementwise maximum of two MLX tensors — mlx_maximum","text":"","code":"mlx_maximum(x, y)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_maximum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Elementwise maximum of two MLX tensors — mlx_maximum","text":"x, y mlx tensors objects coercible as_mlx().","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_maximum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Elementwise maximum of two MLX tensors — mlx_maximum","text":"mlx tensor containing elementwise maximum.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_maximum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Elementwise maximum of two MLX tensors — mlx_maximum","text":"","code":"if (FALSE) { # \\dontrun{ mlx_maximum(1:3, c(3, 2, 1)) } # }"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_minimum.html","id":null,"dir":"Reference","previous_headings":"","what":"Elementwise minimum of two MLX tensors — mlx_minimum","title":"Elementwise minimum of two MLX tensors — mlx_minimum","text":"Elementwise minimum two MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_minimum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Elementwise minimum of two MLX tensors — mlx_minimum","text":"","code":"mlx_minimum(x, y)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_minimum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Elementwise minimum of two MLX tensors — mlx_minimum","text":"x, y mlx tensors objects coercible as_mlx().","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_minimum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Elementwise minimum of two MLX tensors — mlx_minimum","text":"mlx tensor containing elementwise minimum.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_minimum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Elementwise minimum of two MLX tensors — mlx_minimum","text":"","code":"if (FALSE) { # \\dontrun{ a <- as_mlx(matrix(1:4, 2, 2)) b <- as_mlx(matrix(c(4, 3, 2, 1), 2, 2)) mlx_minimum(a, b) } # }"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_moveaxis.html","id":null,"dir":"Reference","previous_headings":"","what":"Reorder MLX tensor axes — mlx_moveaxis","title":"Reorder MLX tensor axes — mlx_moveaxis","text":"mlx_moveaxis() repositions one axes new locations. aperm.mlx() provides familiar R interface, permuting axes according perm via repeated calls mlx_moveaxis().","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_moveaxis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reorder MLX tensor axes — mlx_moveaxis","text":"","code":"mlx_moveaxis(x, source, destination)  # S3 method for class 'mlx' aperm(a, perm = NULL, resize = TRUE, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_moveaxis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reorder MLX tensor axes — mlx_moveaxis","text":"x, object coercible mlx via as_mlx(). source Integer vector axis indices move (1-indexed; negatives count end). destination Integer vector giving target positions source axes (1-indexed; negatives count end). Must length source. perm Integer permutation describing desired axis order, matching semantics base::aperm(). resize Logical flag base::aperm(). TRUE currently supported MLX tensors. ... Additional arguments accepted compatibility; ignored.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_moveaxis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reorder MLX tensor axes — mlx_moveaxis","text":"mlx tensor axes permuted.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_moveaxis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reorder MLX tensor axes — mlx_moveaxis","text":"","code":"x <- as_mlx(array(1:8, dim = c(2, 2, 2))) moved <- mlx_moveaxis(x, source = 1, destination = 3) permuted <- aperm(x, c(2, 1, 3))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_norm.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix and vector norms for MLX tensors — mlx_norm","title":"Matrix and vector norms for MLX tensors — mlx_norm","text":"Matrix vector norms MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_norm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix and vector norms for MLX tensors — mlx_norm","text":"","code":"mlx_norm(x, ord = NULL, axis = NULL, keepdims = FALSE)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_norm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix and vector norms for MLX tensors — mlx_norm","text":"x mlx array. ord Numeric character norm order. Use NULL default 2-norm. axis Optional integer vector axes (1-indexed) along compute norm. keepdims Logical; retain reduced axes length one.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_norm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix and vector norms for MLX tensors — mlx_norm","text":"mlx tensor containing requested norm.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_norm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix and vector norms for MLX tensors — mlx_norm","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_norm(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 5.477226 mlx_norm(x, ord = 2) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 5.464986 mlx_norm(x, axis = 2) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 3.162278 4.472136"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_ones.html","id":null,"dir":"Reference","previous_headings":"","what":"Create tensors of ones on MLX devices — mlx_ones","title":"Create tensors of ones on MLX devices — mlx_ones","text":"Create tensors ones MLX devices","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_ones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create tensors of ones on MLX devices — mlx_ones","text":"","code":"mlx_ones(dim, dtype = c(\"float32\", \"float64\"), device = mlx_default_device())"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_ones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create tensors of ones on MLX devices — mlx_ones","text":"dim Integer vector giving tensor shape. dtype MLX dtype use (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_ones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create tensors of ones on MLX devices — mlx_ones","text":"mlx tensor filled ones.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_ones.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create tensors of ones on MLX devices — mlx_ones","text":"","code":"ones <- mlx_ones(c(2, 2), dtype = \"float64\", device = \"cpu\")"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_optimizer_sgd.html","id":null,"dir":"Reference","previous_headings":"","what":"Stochastic gradient descent optimizer — mlx_optimizer_sgd","title":"Stochastic gradient descent optimizer — mlx_optimizer_sgd","text":"Stochastic gradient descent optimizer","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_optimizer_sgd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stochastic gradient descent optimizer — mlx_optimizer_sgd","text":"","code":"mlx_optimizer_sgd(params, lr = 0.01)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_optimizer_sgd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stochastic gradient descent optimizer — mlx_optimizer_sgd","text":"params List parameters (mlx_parameters()). lr Learning rate.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_optimizer_sgd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stochastic gradient descent optimizer — mlx_optimizer_sgd","text":"optimizer object step() method.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_optimizer_sgd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stochastic gradient descent optimizer — mlx_optimizer_sgd","text":"","code":"set.seed(1) model <- mlx_linear(2, 1, bias = FALSE) opt <- mlx_optimizer_sgd(mlx_parameters(model), lr = 0.1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_pad.html","id":null,"dir":"Reference","previous_headings":"","what":"Pad or split MLX tensors — mlx_pad","title":"Pad or split MLX tensors — mlx_pad","text":"mlx_pad() mirrors MLX padding primitive, enlarging axis according pad_width. Values added symmetrically (pad_width[, 1] , pad_width[, 2] ) using specified mode. mlx_split() divides tensor along axis either equal sections (sections scalar) explicit 1-based split points (sections vector), returning list mlx tensors.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_pad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pad or split MLX tensors — mlx_pad","text":"","code":"mlx_pad(   x,   pad_width,   value = 0,   mode = c(\"constant\", \"edge\", \"reflect\", \"symmetric\"),   axes = NULL )  mlx_split(x, sections, axis = 1L)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_pad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pad or split MLX tensors — mlx_pad","text":"x object coercible mlx via as_mlx(). pad_width Padding extents. Supply single integer, length-two numeric vector, matrix/list one (, ) pair per padded axis. value Constant fill value used mode = \"constant\". mode Padding mode passed MLX (e.g., \"constant\", \"edge\", \"reflect\"). axes Optional integer vector axes (1-indexed, negatives count end) pad_width applies. Unlisted axes receive zero padding. sections Either single integer (number equal parts) integer vector 1-based split points along axis. axis Axis (1-indexed, negatives count end) operate .","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_pad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pad or split MLX tensors — mlx_pad","text":"mlx_pad(), mlx tensor; mlx_split(), list mlx tensors.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_pad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pad or split MLX tensors — mlx_pad","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) padded <- mlx_pad(x, pad_width = 1) padded_cols <- mlx_pad(x, pad_width = c(0, 1), axes = 2) parts <- mlx_split(x, sections = 2, axis = 1) custom_parts <- mlx_split(x, sections = c(1), axis = 2)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_set_values.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign tensors back to parameters — mlx_param_set_values","title":"Assign tensors back to parameters — mlx_param_set_values","text":"Assign tensors back parameters","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_set_values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign tensors back to parameters — mlx_param_set_values","text":"","code":"mlx_param_set_values(params, values)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_set_values.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign tensors back to parameters — mlx_param_set_values","text":"params list mlx_param. values list tensors.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_set_values.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign tensors back to parameters — mlx_param_set_values","text":"","code":"set.seed(1) layer <- mlx_linear(2, 1) params <- mlx_parameters(layer) current <- mlx_param_values(params) mlx_param_set_values(params, current)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_values.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve parameter tensors — mlx_param_values","title":"Retrieve parameter tensors — mlx_param_values","text":"Retrieve parameter tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve parameter tensors — mlx_param_values","text":"","code":"mlx_param_values(params)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_values.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve parameter tensors — mlx_param_values","text":"params list mlx_param.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_values.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve parameter tensors — mlx_param_values","text":"List mlx tensors.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_param_values.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve parameter tensors — mlx_param_values","text":"","code":"set.seed(1) layer <- mlx_linear(2, 1) vals <- mlx_param_values(mlx_parameters(layer))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect parameters from modules — mlx_parameters","title":"Collect parameters from modules — mlx_parameters","text":"Collect parameters modules","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect parameters from modules — mlx_parameters","text":"","code":"mlx_parameters(module)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect parameters from modules — mlx_parameters","text":"module mlx_module list modules.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect parameters from modules — mlx_parameters","text":"list mlx_param objects.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_parameters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect parameters from modules — mlx_parameters","text":"","code":"set.seed(1) layer <- mlx_linear(2, 1) mlx_parameters(layer) #> [[1]] #> $env #> <environment: 0x10f5bb938> #>  #> $name #> [1] \"weight\" #>  #> attr(,\"class\") #> [1] \"mlx_param\" #>  #> [[2]] #> $env #> <environment: 0x10f5bb938> #>  #> $name #> [1] \"bias\" #>  #> attr(,\"class\") #> [1] \"mlx_param\" #>"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_bernoulli.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample Bernoulli random variables on MLX tensors — mlx_rand_bernoulli","title":"Sample Bernoulli random variables on MLX tensors — mlx_rand_bernoulli","text":"Sample Bernoulli random variables MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_bernoulli.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample Bernoulli random variables on MLX tensors — mlx_rand_bernoulli","text":"","code":"mlx_rand_bernoulli(dim, prob = 0.5, device = mlx_default_device())"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_bernoulli.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample Bernoulli random variables on MLX tensors — mlx_rand_bernoulli","text":"dim Integer vector giving tensor shape. prob Probability one. device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_bernoulli.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample Bernoulli random variables on MLX tensors — mlx_rand_bernoulli","text":"mlx boolean tensor.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_bernoulli.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample Bernoulli random variables on MLX tensors — mlx_rand_bernoulli","text":"","code":"mask <- mlx_rand_bernoulli(c(4, 4), prob = 0.3)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_categorical.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from a categorical distribution on MLX tensors — mlx_rand_categorical","title":"Sample from a categorical distribution on MLX tensors — mlx_rand_categorical","text":"Samples indices categorical distributions. row (slice along specified axis) represents separate categorical distribution classes.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_categorical.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from a categorical distribution on MLX tensors — mlx_rand_categorical","text":"","code":"mlx_rand_categorical(logits, axis = -1L, num_samples = 1L)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_categorical.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from a categorical distribution on MLX tensors — mlx_rand_categorical","text":"logits matrix mlx tensor log-probabilities. values need normalized (function applies softmax internally). single distribution K classes, use 1×K matrix. multiple independent distributions, use N×K matrix row distribution. axis axis along sample. Default -1 (last axis, typically class dimension). num_samples Number samples draw distribution.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_categorical.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from a categorical distribution on MLX tensors — mlx_rand_categorical","text":"mlx tensor integer indices (0-indexed) sampled categorical distributions.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_categorical.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from a categorical distribution on MLX tensors — mlx_rand_categorical","text":"","code":"# Single distribution over 3 classes logits <- matrix(c(0.5, 0.2, 0.3), 1, 3) samples <- mlx_rand_categorical(logits, num_samples = 10)  # Multiple distributions logits <- matrix(c(1, 2, 3,                    3, 2, 1), nrow = 2, byrow = TRUE) samples <- mlx_rand_categorical(logits, num_samples = 5)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_gumbel.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from the Gumbel distribution on MLX tensors — mlx_rand_gumbel","title":"Sample from the Gumbel distribution on MLX tensors — mlx_rand_gumbel","text":"Sample Gumbel distribution MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_gumbel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from the Gumbel distribution on MLX tensors — mlx_rand_gumbel","text":"","code":"mlx_rand_gumbel(   dim,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_gumbel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from the Gumbel distribution on MLX tensors — mlx_rand_gumbel","text":"dim Integer vector giving tensor shape. dtype Desired MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_gumbel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from the Gumbel distribution on MLX tensors — mlx_rand_gumbel","text":"mlx tensor Gumbel-distributed entries.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_gumbel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from the Gumbel distribution on MLX tensors — mlx_rand_gumbel","text":"","code":"samples <- mlx_rand_gumbel(c(2, 3))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from the Laplace distribution on MLX tensors — mlx_rand_laplace","title":"Sample from the Laplace distribution on MLX tensors — mlx_rand_laplace","text":"Sample Laplace distribution MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from the Laplace distribution on MLX tensors — mlx_rand_laplace","text":"","code":"mlx_rand_laplace(   dim,   loc = 0,   scale = 1,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from the Laplace distribution on MLX tensors — mlx_rand_laplace","text":"dim Integer vector giving tensor shape. loc Location parameter (mean) Laplace distribution. scale Scale parameter (diversity) Laplace distribution. dtype Desired MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_laplace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from the Laplace distribution on MLX tensors — mlx_rand_laplace","text":"mlx tensor Laplace-distributed entries.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_laplace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from the Laplace distribution on MLX tensors — mlx_rand_laplace","text":"","code":"samples <- mlx_rand_laplace(c(2, 3), loc = 0, scale = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_multivariate_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from a multivariate normal distribution on MLX tensors — mlx_rand_multivariate_normal","title":"Sample from a multivariate normal distribution on MLX tensors — mlx_rand_multivariate_normal","text":"Sample multivariate normal distribution MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_multivariate_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from a multivariate normal distribution on MLX tensors — mlx_rand_multivariate_normal","text":"","code":"mlx_rand_multivariate_normal(   dim,   mean,   cov,   dtype = c(\"float32\", \"float64\"),   device = \"cpu\" )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_multivariate_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from a multivariate normal distribution on MLX tensors — mlx_rand_multivariate_normal","text":"dim Integer vector giving tensor shape. mean mlx tensor vector mean. cov mlx tensor matrix covariance. dtype Desired MLX dtype (\"float32\" \"float64\"). device Target device (\"cpu\" ). Note: function requires CPU due SVD decomposition covariance matrix; GPU device currently supported.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_multivariate_normal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from a multivariate normal distribution on MLX tensors — mlx_rand_multivariate_normal","text":"mlx tensor samples multivariate normal.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_multivariate_normal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from a multivariate normal distribution on MLX tensors — mlx_rand_multivariate_normal","text":"","code":"mean <- as_mlx(c(0, 0), device = \"cpu\") cov <- as_mlx(matrix(c(1, 0, 0, 1), 2, 2), device = \"cpu\") samples <- mlx_rand_multivariate_normal(c(100, 2), mean, cov, device = \"cpu\")"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from a normal distribution on MLX tensors — mlx_rand_normal","title":"Sample from a normal distribution on MLX tensors — mlx_rand_normal","text":"Sample normal distribution MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from a normal distribution on MLX tensors — mlx_rand_normal","text":"","code":"mlx_rand_normal(   dim,   mean = 0,   sd = 1,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from a normal distribution on MLX tensors — mlx_rand_normal","text":"dim Integer vector giving tensor shape. mean Mean normal distribution. sd Standard deviation normal distribution. dtype Desired MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_normal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from a normal distribution on MLX tensors — mlx_rand_normal","text":"mlx tensor normally distributed entries.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_normal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from a normal distribution on MLX tensors — mlx_rand_normal","text":"","code":"weights <- mlx_rand_normal(c(3, 3), mean = 0, sd = 0.1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_permutation.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate random permutations on MLX tensors — mlx_rand_permutation","title":"Generate random permutations on MLX tensors — mlx_rand_permutation","text":"Generate random permutation integers permute entries array along specified axis.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_permutation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate random permutations on MLX tensors — mlx_rand_permutation","text":"","code":"mlx_rand_permutation(x, axis = 0L, device = mlx_default_device())"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_permutation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate random permutations on MLX tensors — mlx_rand_permutation","text":"x Either integer n (generate permutation 0:(n-1)), mlx tensor matrix permute. axis axis along permute x array. Default 0 (permute rows). device Target device (\"gpu\" \"cpu\"). used x integer.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_permutation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate random permutations on MLX tensors — mlx_rand_permutation","text":"mlx tensor containing random permutation.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_permutation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate random permutations on MLX tensors — mlx_rand_permutation","text":"","code":"# Generate a random permutation of 0:9 perm <- mlx_rand_permutation(10)  # Permute the rows of a matrix mat <- matrix(1:12, 4, 3) perm_mat <- mlx_rand_permutation(mat)  # Permute columns instead perm_cols <- mlx_rand_permutation(mat, axis = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_randint.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample random integers on MLX tensors — mlx_rand_randint","title":"Sample random integers on MLX tensors — mlx_rand_randint","text":"Generates random integers uniformly distributed interval [low, high).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_randint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample random integers on MLX tensors — mlx_rand_randint","text":"","code":"mlx_rand_randint(   dim,   low,   high,   dtype = c(\"int32\", \"int64\", \"uint32\", \"uint64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_randint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample random integers on MLX tensors — mlx_rand_randint","text":"dim Integer vector giving tensor shape. low Lower bound (inclusive). high Upper bound (exclusive). dtype Desired integer dtype (\"int32\", \"int64\", \"uint32\", \"uint64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_randint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample random integers on MLX tensors — mlx_rand_randint","text":"mlx tensor random integers.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_randint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample random integers on MLX tensors — mlx_rand_randint","text":"","code":"# Random integers from 0 to 9 samples <- mlx_rand_randint(c(3, 3), low = 0, high = 10)  # Random integers from -5 to 4 samples <- mlx_rand_randint(c(2, 5), low = -5, high = 5)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_truncated_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from a truncated normal distribution on MLX tensors — mlx_rand_truncated_normal","title":"Sample from a truncated normal distribution on MLX tensors — mlx_rand_truncated_normal","text":"Sample truncated normal distribution MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_truncated_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from a truncated normal distribution on MLX tensors — mlx_rand_truncated_normal","text":"","code":"mlx_rand_truncated_normal(   lower,   upper,   dim,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_truncated_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from a truncated normal distribution on MLX tensors — mlx_rand_truncated_normal","text":"lower Lower bound truncated normal. upper Upper bound truncated normal. dim Integer vector giving tensor shape. dtype Desired MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_truncated_normal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from a truncated normal distribution on MLX tensors — mlx_rand_truncated_normal","text":"mlx tensor truncated normally distributed entries.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_truncated_normal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from a truncated normal distribution on MLX tensors — mlx_rand_truncated_normal","text":"","code":"samples <- mlx_rand_truncated_normal(-1, 1, c(5, 5))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from a uniform distribution on MLX tensors — mlx_rand_uniform","title":"Sample from a uniform distribution on MLX tensors — mlx_rand_uniform","text":"Sample uniform distribution MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from a uniform distribution on MLX tensors — mlx_rand_uniform","text":"","code":"mlx_rand_uniform(   dim,   min = 0,   max = 1,   dtype = c(\"float32\", \"float64\"),   device = mlx_default_device() )"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from a uniform distribution on MLX tensors — mlx_rand_uniform","text":"dim Integer vector giving tensor shape. min Lower bound uniform distribution. max Upper bound uniform distribution. dtype Desired MLX dtype (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_uniform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from a uniform distribution on MLX tensors — mlx_rand_uniform","text":"mlx tensor whose entries sampled uniformly.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_rand_uniform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from a uniform distribution on MLX tensors — mlx_rand_uniform","text":"","code":"noise <- mlx_rand_uniform(c(2, 2), min = -1, max = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_reduction_base.html","id":null,"dir":"Reference","previous_headings":"","what":"Shared arguments for MLX/base reduction helpers. — mlx_reduction_base","title":"Shared arguments for MLX/base reduction helpers. — mlx_reduction_base","text":"Shared arguments MLX/base reduction helpers.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_reduction_base.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shared arguments for MLX/base reduction helpers. — mlx_reduction_base","text":"x array mlx tensor. na.rm Logical; currently ignored mlx tensors. dims Dimensions passed base implementation x mlx tensor. ... Additional arguments forwarded base implementation.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Rectified linear activation module — mlx_relu","title":"Rectified linear activation module — mlx_relu","text":"Rectified linear activation module","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rectified linear activation module — mlx_relu","text":"","code":"mlx_relu()"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_relu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rectified linear activation module — mlx_relu","text":"mlx_module applying ReLU.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_relu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rectified linear activation module — mlx_relu","text":"","code":"act <- mlx_relu() x <- as_mlx(matrix(c(-1, 0, 2), 3, 1)) mlx_forward(act, x) #> mlx array [3 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] #> [1,]    0 #> [2,]    0 #> [3,]    2"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_repeat.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeat tensor elements — mlx_repeat","title":"Repeat tensor elements — mlx_repeat","text":"Repeat tensor elements","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_repeat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeat tensor elements — mlx_repeat","text":"","code":"mlx_repeat(x, repeats, axis = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_repeat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeat tensor elements — mlx_repeat","text":"x mlx tensor. repeats Number repetitions. axis Optional axis along repeat. NULL, tensor flattened repetition (matching NumPy semantics).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_repeat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Repeat tensor elements — mlx_repeat","text":"mlx tensor repeated values.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_repeat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Repeat tensor elements — mlx_repeat","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_repeat(x, repeats = 2, axis = 2) #> mlx array [2 x 4] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] [,4] #> [1,]    1    1    3    3 #> [2,]    2    2    4    4"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_roll.html","id":null,"dir":"Reference","previous_headings":"","what":"Roll tensor elements — mlx_roll","title":"Roll tensor elements — mlx_roll","text":"Roll tensor elements","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_roll.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Roll tensor elements — mlx_roll","text":"","code":"mlx_roll(x, shift, axis = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_roll.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Roll tensor elements — mlx_roll","text":"x mlx tensor. shift Integer vector giving number places elements shifted. axis Optional axis (axes) along elements shifted. NULL, tensor flattened shifted.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_roll.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Roll tensor elements — mlx_roll","text":"mlx tensor elements circularly shifted.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_roll.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Roll tensor elements — mlx_roll","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_roll(x, shift = 1, axis = 2) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    3    1 #> [2,]    4    2"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Compose modules sequentially — mlx_sequential","title":"Compose modules sequentially — mlx_sequential","text":"Compose modules sequentially","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compose modules sequentially — mlx_sequential","text":"","code":"mlx_sequential(...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compose modules sequentially — mlx_sequential","text":"... Modules compose.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compose modules sequentially — mlx_sequential","text":"mlx_module.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sequential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compose modules sequentially — mlx_sequential","text":"","code":"set.seed(1) net <- mlx_sequential(mlx_linear(2, 3), mlx_relu(), mlx_linear(3, 1)) input <- as_mlx(matrix(c(1, 2), 1, 2)) mlx_forward(net, input) #> mlx array [1 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>          [,1] #> [1,] 1.419647"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_shape_helpers.html","id":null,"dir":"Reference","previous_headings":"","what":"Tensor shape helpers — mlx_shape_helpers","title":"Tensor shape helpers — mlx_shape_helpers","text":"Tensor shape helpers","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax for MLX tensors — mlx_softmax","title":"Softmax for MLX tensors — mlx_softmax","text":"Softmax MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax for MLX tensors — mlx_softmax","text":"","code":"mlx_softmax(x, axis = NULL, precise = FALSE)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax for MLX tensors — mlx_softmax","text":"x object coercible mlx. axis Optional axis operate (1-indexed like R). NULL, tensor flattened first. precise Logical; compute higher precision stability.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_softmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Softmax for MLX tensors — mlx_softmax","text":"mlx tensor normalized probabilities.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_softmax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Softmax for MLX tensors — mlx_softmax","text":"","code":"x <- as_mlx(matrix(c(1, 2, 3, 4, 5, 6), 2, 3)) sm <- mlx_softmax(x, axis = 2) rowSums(as.matrix(sm)) #> [1] 1 1"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_solve_triangular.html","id":null,"dir":"Reference","previous_headings":"","what":"Solve triangular systems with MLX tensors — mlx_solve_triangular","title":"Solve triangular systems with MLX tensors — mlx_solve_triangular","text":"Solve triangular systems MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_solve_triangular.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Solve triangular systems with MLX tensors — mlx_solve_triangular","text":"","code":"mlx_solve_triangular(a, b, upper = FALSE)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_solve_triangular.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Solve triangular systems with MLX tensors — mlx_solve_triangular","text":"mlx triangular matrix. b Right-hand side matrix vector. upper Logical; TRUE, upper triangular, otherwise lower.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_solve_triangular.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Solve triangular systems with MLX tensors — mlx_solve_triangular","text":"mlx tensor solution.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_solve_triangular.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Solve triangular systems with MLX tensors — mlx_solve_triangular","text":"","code":"a <- as_mlx(matrix(c(2, 1, 0, 3), 2, 2)) b <- as_mlx(matrix(c(1, 5), 2, 1)) mlx_solve_triangular(a, b, upper = FALSE) #> mlx array [2 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] #> [1,]  0.5 #> [2,]  1.5"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sort.html","id":null,"dir":"Reference","previous_headings":"","what":"Sort and argsort for MLX tensors — mlx_sort","title":"Sort and argsort for MLX tensors — mlx_sort","text":"Sort argsort MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sort.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sort and argsort for MLX tensors — mlx_sort","text":"","code":"mlx_sort(x, axis = NULL)  mlx_argsort(x, axis = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sort.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sort and argsort for MLX tensors — mlx_sort","text":"x object coercible mlx. axis Optional axis operate (1-indexed like R). NULL, tensor flattened first.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sort.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sort and argsort for MLX tensors — mlx_sort","text":"mlx tensor containing sorted values (mlx_sort) indices (mlx_argsort).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sort.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sort and argsort for MLX tensors — mlx_sort","text":"Indices returned mlx_argsort() mlx_argpartition() use zero-based offsets, matching MLX's native conventions.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sort.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sort and argsort for MLX tensors — mlx_sort","text":"","code":"x <- as_mlx(c(3, 1, 4, 2)) mlx_sort(x) #> mlx array [4] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1 2 3 4 mlx_argsort(x) #> mlx array [4] #>   dtype: uint32 #>   device: gpu #>   values: #> [1] 1 3 0 2 mlx_sort(as_mlx(matrix(1:6, 2, 3)), axis = 1) #> mlx array [2 x 3] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] #> [1,]    1    3    5 #> [2,]    2    4    6"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_squeeze.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove singleton dimensions — mlx_squeeze","title":"Remove singleton dimensions — mlx_squeeze","text":"Remove singleton dimensions","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_squeeze.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove singleton dimensions — mlx_squeeze","text":"","code":"mlx_squeeze(x, axis = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_squeeze.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove singleton dimensions — mlx_squeeze","text":"x mlx tensor. axis Optional integer vector axes (1-indexed) remove. NULL axes length one removed.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_squeeze.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove singleton dimensions — mlx_squeeze","text":"mlx tensor selected axes removed.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_squeeze.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove singleton dimensions — mlx_squeeze","text":"","code":"x <- as_mlx(array(1:4, dim = c(1, 2, 2, 1))) mlx_squeeze(x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4 mlx_squeeze(x, axis = 1) #> mlx array [2 x 2 x 1] #>   dtype: float32 #>   device: gpu #>   (4 elements, not shown)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stack.html","id":null,"dir":"Reference","previous_headings":"","what":"Stack MLX tensors along a new axis — mlx_stack","title":"Stack MLX tensors along a new axis — mlx_stack","text":"Stack MLX tensors along new axis","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stack MLX tensors along a new axis — mlx_stack","text":"","code":"mlx_stack(..., axis = 1L)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stack MLX tensors along a new axis — mlx_stack","text":"... One tensors (single list tensors) coercible mlx. axis Position new axis (1-indexed, negative values count end).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stack MLX tensors along a new axis — mlx_stack","text":"mlx tensor one additional dimension.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stack.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stack MLX tensors along a new axis — mlx_stack","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) y <- as_mlx(matrix(5:8, 2, 2)) stacked <- mlx_stack(x, y, axis = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stop_gradient.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop gradient propagation through an MLX tensor — mlx_stop_gradient","title":"Stop gradient propagation through an MLX tensor — mlx_stop_gradient","text":"Stop gradient propagation MLX tensor","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stop_gradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop gradient propagation through an MLX tensor — mlx_stop_gradient","text":"","code":"mlx_stop_gradient(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stop_gradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop gradient propagation through an MLX tensor — mlx_stop_gradient","text":"x mlx tensor.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stop_gradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stop gradient propagation through an MLX tensor — mlx_stop_gradient","text":"new mlx tensor identical values zero gradient.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_stop_gradient.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stop gradient propagation through an MLX tensor — mlx_stop_gradient","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_stop_gradient(x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sum.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce MLX tensors — mlx_sum","title":"Reduce MLX tensors — mlx_sum","text":"helpers mirror NumPy-style reductions, optionally collapsing one axes. Use drop = FALSE retain reduced axes length one (akin keepdims = TRUE NumPy).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce MLX tensors — mlx_sum","text":"","code":"mlx_sum(x, axis = NULL, drop = TRUE)  mlx_prod(x, axis = NULL, drop = TRUE)  mlx_all(x, axis = NULL, drop = TRUE)  mlx_any(x, axis = NULL, drop = TRUE)  mlx_mean(x, axis = NULL, drop = TRUE)  mlx_var(x, axis = NULL, drop = TRUE, ddof = 0L)  mlx_std(x, axis = NULL, drop = TRUE, ddof = 0L)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce MLX tensors — mlx_sum","text":"x object coercible mlx via as_mlx(). axis Optional integer vector axes (1-indexed) reduce. NULL, reduction performed elements. drop Logical flag controlling dimension dropping: TRUE (default) removes reduced axes, FALSE retains length one. ddof Non-negative integer delta degrees freedom variance standard deviation reductions.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce MLX tensors — mlx_sum","text":"mlx tensor containing reduction result.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_sum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reduce MLX tensors — mlx_sum","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_sum(x) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 10 mlx_sum(x, axis = 1) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 3 7 mlx_prod(x, axis = 2, drop = FALSE) #> mlx array [2 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] #> [1,]    3 #> [2,]    8 mlx_all(x > 0) #> mlx array [] #>   dtype: bool #>   device: gpu #>   values: #> [1] TRUE mlx_any(x > 3) #> mlx array [] #>   dtype: bool #>   device: gpu #>   values: #> [1] TRUE mlx_mean(x, axis = 1) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1.5 3.5 mlx_var(x, axis = 2) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1 1 mlx_std(x, ddof = 1) #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 1.290994"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_synchronize.html","id":null,"dir":"Reference","previous_headings":"","what":"Synchronize MLX device — mlx_synchronize","title":"Synchronize MLX device — mlx_synchronize","text":"Waits outstanding operations specified device complete.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_synchronize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synchronize MLX device — mlx_synchronize","text":"","code":"mlx_synchronize(device = c(\"gpu\", \"cpu\"))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_synchronize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synchronize MLX device — mlx_synchronize","text":"device Device synchronize (\"gpu\" \"cpu\").","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_synchronize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Synchronize MLX device — mlx_synchronize","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_synchronize(\"gpu\")"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_tile.html","id":null,"dir":"Reference","previous_headings":"","what":"Tile a tensor — mlx_tile","title":"Tile a tensor — mlx_tile","text":"Tile tensor","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_tile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tile a tensor — mlx_tile","text":"","code":"mlx_tile(x, reps)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_tile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tile a tensor — mlx_tile","text":"x mlx tensor. reps Integer vector giving number repetitions axis.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_tile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tile a tensor — mlx_tile","text":"mlx tensor tiled content.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_tile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tile a tensor — mlx_tile","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) mlx_tile(x, reps = c(1, 2)) #> mlx array [2 x 4] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] [,4] #> [1,]    1    3    1    3 #> [2,]    2    4    2    4"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_topk.html","id":null,"dir":"Reference","previous_headings":"","what":"Top-k selection and partitioning on MLX tensors — mlx_topk","title":"Top-k selection and partitioning on MLX tensors — mlx_topk","text":"Top-k selection partitioning MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_topk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Top-k selection and partitioning on MLX tensors — mlx_topk","text":"","code":"mlx_topk(x, k, axis = NULL)  mlx_partition(x, kth, axis = NULL)  mlx_argpartition(x, kth, axis = NULL)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_topk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Top-k selection and partitioning on MLX tensors — mlx_topk","text":"x object coercible mlx. k Positive integer specifying number elements select. axis Optional axis operate (1-indexed like R). NULL, tensor flattened first. kth Zero-based index element placed -order partitioning.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_topk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Top-k selection and partitioning on MLX tensors — mlx_topk","text":"mlx tensor.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_topk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Top-k selection and partitioning on MLX tensors — mlx_topk","text":"mlx_topk() returns largest k values reported MLX. Use mlx_argsort() need associated indices.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_topk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Top-k selection and partitioning on MLX tensors — mlx_topk","text":"","code":"scores <- as_mlx(c(0.7, 0.2, 0.9, 0.4)) mlx_topk(scores, k = 2) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 0.7 0.9 mlx_partition(scores, kth = 1) #> mlx array [4] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 0.2 0.4 0.7 0.9 mlx_argpartition(scores, kth = 1) #> mlx array [4] #>   dtype: uint32 #>   device: gpu #>   values: #> [1] 1 3 0 2 mlx_topk(as_mlx(matrix(1:6, 2, 3)), k = 1, axis = 1) #> mlx array [1 x 3] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] #> [1,]    2    4    6"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_train_step.html","id":null,"dir":"Reference","previous_headings":"","what":"Single training step helper — mlx_train_step","title":"Single training step helper — mlx_train_step","text":"Single training step helper","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_train_step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single training step helper — mlx_train_step","text":"","code":"mlx_train_step(module, loss_fn, optimizer, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_train_step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single training step helper — mlx_train_step","text":"module mlx_module. loss_fn Function module data returning mlx scalar. optimizer Optimizer object mlx_optimizer_sgd(). ... Additional data passed loss_fn.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_train_step.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single training step helper — mlx_train_step","text":"list current loss.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_train_step.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single training step helper — mlx_train_step","text":"","code":"set.seed(1) model <- mlx_linear(2, 1, bias = FALSE) opt <- mlx_optimizer_sgd(mlx_parameters(model), lr = 0.1) data_x <- as_mlx(matrix(c(1, 2, 3, 4), 2, 2)) data_y <- as_mlx(matrix(c(1, 2), 2, 1)) loss_fn <- function(mod, x, y) {   preds <- mlx_forward(mod, x)   diff <- preds - y   sum(diff * diff) } mlx_train_step(model, loss_fn, opt, data_x, data_y) #> $loss #> mlx array [] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 7.49876 #>"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_where.html","id":null,"dir":"Reference","previous_headings":"","what":"Elementwise conditional selection — mlx_where","title":"Elementwise conditional selection — mlx_where","text":"Elementwise conditional selection","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_where.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Elementwise conditional selection — mlx_where","text":"","code":"mlx_where(condition, x, y)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_where.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Elementwise conditional selection — mlx_where","text":"condition Logical mlx tensor (non-zero values treated TRUE). x, y Tensors broadcastable shape condition.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_where.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Elementwise conditional selection — mlx_where","text":"mlx tensor elements drawn x condition TRUE, otherwise y.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_where.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Elementwise conditional selection — mlx_where","text":"Behaves like ifelse() tensors, evaluates branches.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_where.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Elementwise conditional selection — mlx_where","text":"","code":"cond <- as_mlx(matrix(c(TRUE, FALSE, TRUE, FALSE), 2, 2)) a <- as_mlx(matrix(1:4, 2, 2)) b <- as_mlx(matrix(5:8, 2, 2)) mlx_where(cond, a, b) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    1    3 #> [2,]    6    8"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_zeros.html","id":null,"dir":"Reference","previous_headings":"","what":"Create tensors of zeros on MLX devices — mlx_zeros","title":"Create tensors of zeros on MLX devices — mlx_zeros","text":"Create tensors zeros MLX devices","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_zeros.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create tensors of zeros on MLX devices — mlx_zeros","text":"","code":"mlx_zeros(dim, dtype = c(\"float32\", \"float64\"), device = mlx_default_device())"},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_zeros.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create tensors of zeros on MLX devices — mlx_zeros","text":"dim Integer vector giving tensor shape. dtype MLX dtype use (\"float32\" \"float64\"). device Target device (\"gpu\" \"cpu\").","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_zeros.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create tensors of zeros on MLX devices — mlx_zeros","text":"mlx tensor filled zeros.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/mlx_zeros.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create tensors of zeros on MLX devices — mlx_zeros","text":"","code":"zeros <- mlx_zeros(c(2, 3))"},{"path":"https://hughjonesd.github.io/Rmlx/reference/pinv.html","id":null,"dir":"Reference","previous_headings":"","what":"Moore-Penrose pseudoinverse for MLX arrays — pinv","title":"Moore-Penrose pseudoinverse for MLX arrays — pinv","text":"Moore-Penrose pseudoinverse MLX arrays","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/pinv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Moore-Penrose pseudoinverse for MLX arrays — pinv","text":"","code":"pinv(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/pinv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Moore-Penrose pseudoinverse for MLX arrays — pinv","text":"x mlx object coercible matrix.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/pinv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Moore-Penrose pseudoinverse for MLX arrays — pinv","text":"mlx object containing pseudoinverse.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/pinv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Moore-Penrose pseudoinverse for MLX arrays — pinv","text":"","code":"x <- as_mlx(matrix(c(1, 2, 3, 4), 2, 2)) pinv(x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1]       [,2] #> [1,]   -2  1.5000004 #> [2,]    1 -0.5000001"},{"path":"https://hughjonesd.github.io/Rmlx/reference/print.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Print MLX array — print.mlx","title":"Print MLX array — print.mlx","text":"Print MLX array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/print.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print MLX array — print.mlx","text":"","code":"# S3 method for class 'mlx' print(x, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/print.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print MLX array — print.mlx","text":"x mlx object ... Additional arguments (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/print.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print MLX array — print.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) print(x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4"},{"path":"https://hughjonesd.github.io/Rmlx/reference/qr.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"QR decomposition for MLX tensors — qr.mlx","title":"QR decomposition for MLX tensors — qr.mlx","text":"QR decomposition MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/qr.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"QR decomposition for MLX tensors — qr.mlx","text":"","code":"# S3 method for class 'mlx' qr(x, tol = 1e-07, LAPACK = FALSE, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/qr.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"QR decomposition for MLX tensors — qr.mlx","text":"x mlx matrix. tol Ignored; custom tolerances supported. LAPACK Ignored; set FALSE. ... Additional arguments (unused).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/qr.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"QR decomposition for MLX tensors — qr.mlx","text":"list components Q R, mlx matrix.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/qr.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"QR decomposition for MLX tensors — qr.mlx","text":"","code":"x <- as_mlx(matrix(c(1, 2, 3, 4, 5, 6), 3, 2)) qr(x) #> $Q #> mlx array [3 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>            [,1]       [,2] #> [1,] -0.2672611  0.8728715 #> [2,] -0.5345225  0.2182179 #> [3,] -0.8017837 -0.4364358 #>  #> $R #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>           [,1]      [,2] #> [1,] -3.741657 -8.552359 #> [2,]  0.000000  1.963961 #>  #> attr(,\"class\") #> [1] \"mlx_qr\" \"list\""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rbind.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Row-bind MLX tensors — rbind.mlx","title":"Row-bind MLX tensors — rbind.mlx","text":"Row-bind MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rbind.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Row-bind MLX tensors — rbind.mlx","text":"","code":"# S3 method for class 'mlx' rbind(..., deparse.level = 1)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/rbind.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Row-bind MLX tensors — rbind.mlx","text":"... Objects bind. MLX tensors kept MLX; inputs coerced via as_mlx(). deparse.level Compatibility argument accepted S3 dispatch; ignored.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rbind.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Row-bind MLX tensors — rbind.mlx","text":"mlx tensor stacked along first axis.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/rbind.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Row-bind MLX tensors — rbind.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) y <- as_mlx(matrix(5:8, 2, 2)) rbind(x, y) #> mlx array [4 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4 #> [3,]    5    7 #> [4,]    6    8"},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowMeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Row means for MLX tensors — rowMeans","title":"Row means for MLX tensors — rowMeans","text":"Row means MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowMeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Row means for MLX tensors — rowMeans","text":"","code":"rowMeans(x, na.rm = FALSE, dims = 1, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowMeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Row means for MLX tensors — rowMeans","text":"x array mlx tensor. na.rm Logical; currently ignored mlx tensors. dims Dimensions passed base implementation x mlx tensor. ... Additional arguments forwarded base implementation.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowMeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Row means for MLX tensors — rowMeans","text":"mlx tensor x mlx, otherwise numeric vector.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowMeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Row means for MLX tensors — rowMeans","text":"","code":"x <- as_mlx(matrix(1:6, 3, 2)) rowMeans(x) #> mlx array [3] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 2.5 3.5 4.5"},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowSums.html","id":null,"dir":"Reference","previous_headings":"","what":"Row sums for MLX tensors — rowSums","title":"Row sums for MLX tensors — rowSums","text":"Row sums MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowSums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Row sums for MLX tensors — rowSums","text":"","code":"rowSums(x, na.rm = FALSE, dims = 1, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowSums.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Row sums for MLX tensors — rowSums","text":"x array mlx tensor. na.rm Logical; currently ignored mlx tensors. dims Dimensions passed base implementation x mlx tensor. ... Additional arguments forwarded base implementation.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowSums.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Row sums for MLX tensors — rowSums","text":"mlx tensor x mlx, otherwise numeric vector.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/rowSums.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Row sums for MLX tensors — rowSums","text":"","code":"x <- as_mlx(matrix(1:6, 3, 2)) rowSums(x) #> mlx array [3] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 5 7 9"},{"path":"https://hughjonesd.github.io/Rmlx/reference/solve.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Solve a system of linear equations — solve.mlx","title":"Solve a system of linear equations — solve.mlx","text":"Solve system linear equations","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/solve.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Solve a system of linear equations — solve.mlx","text":"","code":"# S3 method for class 'mlx' solve(a, b = NULL, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/solve.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Solve a system of linear equations — solve.mlx","text":"mlx matrix (coefficient matrix) b mlx vector matrix (right-hand side). omitted, computes matrix inverse. ... Additional arguments (compatibility base::solve)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/solve.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Solve a system of linear equations — solve.mlx","text":"mlx object containing solution","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/solve.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Solve a system of linear equations — solve.mlx","text":"","code":"a <- as_mlx(matrix(c(3, 1, 1, 2), 2, 2)) b <- as_mlx(c(9, 8)) solve(a, b) #> mlx array [2] #>   dtype: float32 #>   device: gpu #>   values: #> [1] 2 3"},{"path":"https://hughjonesd.github.io/Rmlx/reference/str.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Object structure for MLX array — str.mlx","title":"Object structure for MLX array — str.mlx","text":"Object structure MLX array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/str.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Object structure for MLX array — str.mlx","text":"","code":"# S3 method for class 'mlx' str(object, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/str.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Object structure for MLX array — str.mlx","text":"object mlx object ... Additional arguments (ignored)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/str.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Object structure for MLX array — str.mlx","text":"","code":"x <- as_mlx(matrix(1:4, 2, 2)) str(x) #> mlx [2 x 2] float32 on gpu"},{"path":"https://hughjonesd.github.io/Rmlx/reference/sub-.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Subset MLX array — [.mlx","title":"Subset MLX array — [.mlx","text":"Subset MLX array","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/sub-.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subset MLX array — [.mlx","text":"","code":"# S3 method for class 'mlx' x[..., drop = FALSE]"},{"path":"https://hughjonesd.github.io/Rmlx/reference/sub-.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subset MLX array — [.mlx","text":"x mlx object ... Indices dimension. Provide one per axis; omitted indices select full extent. Logical indices recycle dimension length. drop dimensions dropped? (default: FALSE)","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/sub-.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subset MLX array — [.mlx","text":"Subsetted mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/sub-.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subset MLX array — [.mlx","text":"","code":"x <- as_mlx(matrix(1:9, 3, 3)) x[1, ] #> mlx array [1 x 3] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] [,3] #> [1,]    1    4    7"},{"path":"https://hughjonesd.github.io/Rmlx/reference/svd.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Singular value decomposition for MLX tensors — svd.mlx","title":"Singular value decomposition for MLX tensors — svd.mlx","text":"Singular value decomposition MLX tensors","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/svd.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Singular value decomposition for MLX tensors — svd.mlx","text":"","code":"# S3 method for class 'mlx' svd(x, nu = min(n, p), nv = min(n, p), LINPACK = FALSE, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/svd.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Singular value decomposition for MLX tensors — svd.mlx","text":"x mlx matrix. nu Number left singular vectors return (0 min(dim(x))). nv Number right singular vectors return (0 min(dim(x))). LINPACK Ignored; set FALSE. ... Additional arguments (unused).","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/svd.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Singular value decomposition for MLX tensors — svd.mlx","text":"list components d, u, v.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/svd.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Singular value decomposition for MLX tensors — svd.mlx","text":"","code":"x <- as_mlx(matrix(c(1, 0, 0, 2), 2, 2)) svd(x) #> $d #> [1] 2 1 #>  #> $u #>      [,1] [,2] #> [1,]    0    1 #> [2,]    1    0 #>  #> $v #>      [,1] [,2] #> [1,]    0    1 #> [2,]    1    0 #>"},{"path":"https://hughjonesd.github.io/Rmlx/reference/t.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Transpose of MLX matrix — t.mlx","title":"Transpose of MLX matrix — t.mlx","text":"Transpose MLX matrix","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/t.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transpose of MLX matrix — t.mlx","text":"","code":"# S3 method for class 'mlx' t(x)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/t.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transpose of MLX matrix — t.mlx","text":"x mlx matrix","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/t.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transpose of MLX matrix — t.mlx","text":"Transposed mlx matrix","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/t.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transpose of MLX matrix — t.mlx","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) t(x) #> mlx array [3 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]    1    2 #> [2,]    3    4 #> [3,]    5    6"},{"path":"https://hughjonesd.github.io/Rmlx/reference/tcrossprod.mlx.html","id":null,"dir":"Reference","previous_headings":"","what":"Transposed cross product — tcrossprod.mlx","title":"Transposed cross product — tcrossprod.mlx","text":"Transposed cross product","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/tcrossprod.mlx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transposed cross product — tcrossprod.mlx","text":"","code":"# S3 method for class 'mlx' tcrossprod(x, y = NULL, ...)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/tcrossprod.mlx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transposed cross product — tcrossprod.mlx","text":"x mlx matrix y mlx matrix (default: NULL, uses x) ... Additional arguments passed base::tcrossprod.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/tcrossprod.mlx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transposed cross product — tcrossprod.mlx","text":"x %*% t(y) mlx object","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/tcrossprod.mlx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transposed cross product — tcrossprod.mlx","text":"","code":"x <- as_mlx(matrix(1:6, 2, 3)) tcrossprod(x) #> mlx array [2 x 2] #>   dtype: float32 #>   device: gpu #>   values: #>      [,1] [,2] #> [1,]   35   44 #> [2,]   44   56"},{"path":"https://hughjonesd.github.io/Rmlx/reference/with_default_device.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporarily set the default MLX device — with_default_device","title":"Temporarily set the default MLX device — with_default_device","text":"Temporarily set default MLX device","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/with_default_device.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporarily set the default MLX device — with_default_device","text":"","code":"with_default_device(device = c(\"gpu\", \"cpu\"), code)"},{"path":"https://hughjonesd.github.io/Rmlx/reference/with_default_device.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Temporarily set the default MLX device — with_default_device","text":"device Device use (\"gpu\" \"cpu\"). code Expression evaluate device active.","code":""},{"path":"https://hughjonesd.github.io/Rmlx/reference/with_default_device.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Temporarily set the default MLX device — with_default_device","text":"result evaluating code.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/Rmlx/reference/with_default_device.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Temporarily set the default MLX device — with_default_device","text":"","code":"old <- mlx_default_device() with_default_device(\"cpu\", mlx_default_device()) #> [1] \"cpu\" mlx_default_device(old) #> [1] \"gpu\""}]
